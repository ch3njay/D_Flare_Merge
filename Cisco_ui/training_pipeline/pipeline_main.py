"""Cisco ASA Training Pipeline - æ•´åˆè¨“ç·´æµç¨‹ä¸»ç¨‹å¼ã€‚"""
from __future__ import annotations

import json
import os
import re
import time
import joblib
from typing import Any, Dict, Tuple
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score
)

# å°å…¥ Cisco è¨“ç·´æ¨¡çµ„
from .model_builder import ModelBuilder
from .evaluator import Evaluator
from .combo_optimizer import ComboOptimizer


class CiscoTrainingPipeline:
    """Cisco ASA æ©Ÿå™¨å­¸ç¿’è¨“ç·´ç®¡ç·š"""
    
    def __init__(
        self,
        task_type: str = "binary",
        config: Dict[str, Any] | None = None,
    ):
        """
        åˆå§‹åŒ–è¨“ç·´ç®¡ç·š
        
        Args:
            task_type: è¨“ç·´ä»»å‹™é¡å‹ ("binary" æˆ– "multiclass")
            config: è‡ªè¨‚é…ç½®å­—å…¸
        """
        self.task_type = task_type
        self.config = config or {}
        
        # è¨­å®šé è¨­å€¼
        self.config.setdefault("test_size", 0.2)
        self.config.setdefault("random_state", 42)
        self.config.setdefault("output_dir", "./artifacts")
        
        # åˆå§‹åŒ–æ¨¡çµ„
        self.model_builder = ModelBuilder()
        self.evaluator = Evaluator()
        
        # è¼¸å‡ºç›®éŒ„
        self.out_dir = None
        
    def _prepare_artifacts_dir(self) -> str:
        """å»ºç«‹è¼¸å‡ºç›®éŒ„"""
        root = self.config.get("output_dir", "./artifacts")
        ts = time.strftime("%Y%m%d_%H%M%S")
        out_dir = os.path.join(root, ts)
        os.makedirs(os.path.join(out_dir, "models"), exist_ok=True)
        os.makedirs(os.path.join(out_dir, "reports"), exist_ok=True)
        return out_dir
    
    def _load_data(self, csv_path: str) -> pd.DataFrame:
        """è¼‰å…¥è¨“ç·´è³‡æ–™ - æ”¯æŒæ ¼å¼è‡ªå‹•åµæ¸¬"""
        print(f"ğŸ“‚ è¼‰å…¥è³‡æ–™ï¼š{csv_path}")
        
        # åµæ¸¬è³‡æ–™æ ¼å¼
        data_format = self._detect_data_format(csv_path)
        print(f"ğŸ” åµæ¸¬åˆ°è³‡æ–™æ ¼å¼ï¼š{data_format}")
        
        if data_format == "compressed":
            print("ğŸ“¦ åµæ¸¬åˆ°å£“ç¸®æª”æ¡ˆï¼Œå…ˆè§£å£“ç¸®...")
            extracted_file = self._extract_compressed_file(csv_path)
            if extracted_file:
                # éè¿´æª¢æ¸¬è§£å£“å¾Œçš„æª”æ¡ˆæ ¼å¼
                return self._load_data(extracted_file)
            else:
                print("âŒ å£“ç¸®æª”æ¡ˆè§£å£“å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è§£æ...")
                df = self._load_csv_with_fallback(csv_path)
        elif data_format == "cisco_asa_log":
            print("ğŸ”„ åµæ¸¬åˆ° Cisco ASA Log æ ¼å¼ï¼ŒåŸ·è¡Œ ETL å‰è™•ç†...")
            df = self._process_cisco_logs(csv_path)
        elif data_format == "csv":
            df = self._load_csv_with_fallback(csv_path)
        else:
            print(f"âš ï¸ æœªçŸ¥æ ¼å¼ {data_format}ï¼Œå˜—è©¦ä½¿ç”¨ CSV è§£æ...")
            df = self._load_csv_with_fallback(csv_path)
        
        # åŸºæœ¬ ETL è½‰æ›
        df = self._apply_basic_etl_transforms(df)
        
        print(f"âœ… è³‡æ–™ç­†æ•¸ï¼š{len(df)} rows, {len(df.columns)} columns")
        return df
    
    def _detect_data_format(self, file_path: str) -> str:
        """åµæ¸¬è¼¸å…¥æª”æ¡ˆçš„è³‡æ–™æ ¼å¼ï¼Œæ”¯æ´å£“ç¸®æª”æ¡ˆ"""
        import os
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå£“ç¸®æª”æ¡ˆ
        file_extension = os.path.splitext(file_path)[1].lower()
        if file_extension in ['.gz', '.zip', '.7z', '.rar', '.tar', '.bz2']:
            return "compressed"
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                sample_lines = [f.readline().strip() for _ in range(10)]
            
            # ç§»é™¤ç©ºè¡Œ
            sample_lines = [line for line in sample_lines if line]
            if not sample_lines:
                return "empty"
            
            # æª¢æŸ¥æ˜¯å¦ç‚º Cisco ASA log æ ¼å¼
            cisco_patterns = [
                r'%ASA-\d+-\d+:',           # ASA syslog æ ¼å¼
                r'Connection\s+\w+',        # Connection built/teardown
                r'Deny\s+\w+',              # Deny è¦å‰‡
                r'Built\s+\w+',             # Built connection
                r'Teardown\s+\w+',          # Teardown connection
            ]
            
            cisco_matches = 0
            for line in sample_lines:
                for pattern in cisco_patterns:
                    if re.search(pattern, line):
                        cisco_matches += 1
                        break
            
            # å¦‚æœè¶…é30%çš„è¡ŒåŒ¹é… Cisco æ¨¡å¼
            if cisco_matches / len(sample_lines) > 0.3:
                return "cisco_asa_log"
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ¨™æº– CSVï¼ˆæœ‰é€—è™Ÿä¸”ç¬¬ä¸€è¡Œåƒæ¨™é¡Œï¼‰
            first_line = sample_lines[0]
            if ',' in first_line and not first_line.startswith('%'):
                # å˜—è©¦è§£æ CSV æ¨™é¡Œ
                potential_headers = [h.strip() for h in first_line.split(',')]
                if (len(potential_headers) >= 3 and 
                    all(h.isalnum() or '_' in h for h in potential_headers[:3])):
                    return "csv"
            
            return "unknown"
            
        except Exception as e:
            print(f"âš ï¸ æ ¼å¼åµæ¸¬å¤±æ•—ï¼š{e}")
            return "unknown"
    
    def _extract_compressed_file(self, file_path: str) -> str | None:
        """è§£å£“ç¸®æª”æ¡ˆä¸¦è¿”å›è§£å£“å¾Œçš„æª”æ¡ˆè·¯å¾‘"""
        import os
        import tempfile
        import zipfile
        import gzip
        import tarfile
        import shutil
        
        file_extension = os.path.splitext(file_path)[1].lower()
        temp_dir = tempfile.mkdtemp()
        
        try:
            if file_extension == '.gz':
                # è™•ç† .gz æª”æ¡ˆ
                output_path = os.path.join(temp_dir, 'extracted_file.txt')
                with gzip.open(file_path, 'rt', encoding='utf-8', 
                               errors='ignore') as gz_file:
                    with open(output_path, 'w', encoding='utf-8') as out_file:
                        shutil.copyfileobj(gz_file, out_file)
                print(f"âœ… æˆåŠŸè§£å£“ .gz æª”æ¡ˆåˆ°: {output_path}")
                return output_path
                
            elif file_extension == '.zip':
                # è™•ç† .zip æª”æ¡ˆ
                with zipfile.ZipFile(file_path, 'r') as zip_file:
                    # å–å¾—ç¬¬ä¸€å€‹æª”æ¡ˆ
                    file_list = zip_file.namelist()
                    if not file_list:
                        print("âŒ ZIP æª”æ¡ˆç‚ºç©º")
                        return None
                    
                    first_file = file_list[0]
                    extracted_path = zip_file.extract(first_file, temp_dir)
                    print(f"âœ… æˆåŠŸè§£å£“ .zip æª”æ¡ˆåˆ°: {extracted_path}")
                    return extracted_path
                    
            elif file_extension in ['.tar', '.tar.gz', '.tgz']:
                # è™•ç† .tar ç›¸é—œæª”æ¡ˆ
                with tarfile.open(file_path, 'r:*') as tar_file:
                    members = tar_file.getmembers()
                    if not members:
                        print("âŒ TAR æª”æ¡ˆç‚ºç©º")
                        return None
                    
                    first_member = members[0]
                    if first_member.isfile():
                        tar_file.extract(first_member, temp_dir)
                        extracted_path = os.path.join(
                            temp_dir, first_member.name
                        )
                        print(f"âœ… æˆåŠŸè§£å£“ .tar æª”æ¡ˆåˆ°: {extracted_path}")
                        return extracted_path
                        
            else:
                print(f"âš ï¸ ä¸æ”¯æ´çš„å£“ç¸®æ ¼å¼: {file_extension}")
                return None
                
        except Exception as e:
            print(f"âŒ è§£å£“ç¸®å¤±æ•—: {e}")
            # æ¸…ç†æš«å­˜ç›®éŒ„
            try:
                shutil.rmtree(temp_dir)
            except Exception:
                pass
            return None
    
    def _process_cisco_logs(self, log_path: str) -> pd.DataFrame:
        """è™•ç† Cisco ASA æ—¥èªŒæª”æ¡ˆï¼Œè½‰æ›ç‚ºçµæ§‹åŒ–è³‡æ–™"""
        # å°å…¥ ETL æ¨¡çµ„
        try:
            from ..etl_pipeline.log_cleaning import step1_process_logs
            from ..etl_pipeline.log_mapping import step2_apply_mappings
            from ..etl_pipeline.feature_engineering import step3_generate_features
        except ImportError as e:
            print(f"âŒ ç„¡æ³•å°å…¥ ETL æ¨¡çµ„ï¼š{e}")
            print("ğŸ“„ å˜—è©¦ç›´æ¥è®€å–ç‚º CSV...")
            return self._load_csv_with_fallback(log_path)
        
        import tempfile
        import os
        
        # å»ºç«‹æš«å­˜ç›®éŒ„
        with tempfile.TemporaryDirectory() as temp_dir:
            step1_out = os.path.join(temp_dir, "step1_cleaned.csv")
            step2_out = os.path.join(temp_dir, "step2_mapped.csv")
            step3_out = os.path.join(temp_dir, "step3_features.csv")
            unique_json = os.path.join(temp_dir, "unique.json")
            mappings_json = os.path.join(temp_dir, "mappings.json")
            
            try:
                # Step 1: æ¸…æ´—åŸå§‹æ—¥èªŒ
                print("  ğŸ“‹ Step 1: æ¸…æ´—åŸå§‹æ—¥èªŒ...")
                step1_process_logs(log_path, step1_out, unique_json, batch_id=1, show_progress=False)
                
                # Step 2: æ‡‰ç”¨æ˜ å°„
                print("  ğŸ—ºï¸ Step 2: æ‡‰ç”¨æ¬„ä½æ˜ å°„...")
                step2_apply_mappings(step1_out, step2_out, mappings_json, show_progress=False)
                
                # Step 3: ç‰¹å¾µå·¥ç¨‹
                print("  âš™ï¸ Step 3: ç‰¹å¾µå·¥ç¨‹...")
                step3_generate_features(step2_out, step3_out, mappings_json, show_progress=False)
                
                # è¼‰å…¥æœ€çµ‚çµæœ
                df = pd.read_csv(step3_out)
                print(f"  âœ… ETL è™•ç†å®Œæˆï¼Œç”¢ç”Ÿ {len(df)} ç­†çµæ§‹åŒ–è³‡æ–™")
                return df
                
            except Exception as e:
                print(f"  âŒ ETL è™•ç†å¤±æ•—ï¼š{e}")
                print("  ğŸ“„ æ”¹ç”¨ç›´æ¥ CSV è®€å–...")
                return self._load_csv_with_fallback(log_path)
    
    def _load_csv_with_fallback(self, csv_path: str) -> pd.DataFrame:
        """ä½¿ç”¨å¤šç¨®æ–¹æ³•è¼‰å…¥ CSV æª”æ¡ˆ"""
        try:
            # æ–¹æ³• 1: æ¨™æº–è®€å–
            return pd.read_csv(csv_path)
        except pd.errors.ParserError:
            try:
                # æ–¹æ³• 2: ä½¿ç”¨ Python å¼•æ“ï¼Œè·³ééŒ¯èª¤è¡Œ
                print("âš ï¸ CSV è§£æéŒ¯èª¤ï¼Œä½¿ç”¨ Python å¼•æ“é‡è©¦...")
                return pd.read_csv(
                    csv_path, engine='python', 
                    error_bad_lines=False, warn_bad_lines=False
                )
            except Exception:
                try:
                    # æ–¹æ³• 3: æ›´å¯¬é¬†çš„è§£æ
                    print("âš ï¸ ä½¿ç”¨æ›´å¯¬é¬†çš„ CSV è§£æ...")
                    return pd.read_csv(
                        csv_path, sep=None, engine='python', 
                        encoding='utf-8', on_bad_lines='skip'
                    )
                except Exception as e:
                    print(f"âŒ æ‰€æœ‰ CSV è§£ææ–¹æ³•éƒ½å¤±æ•—ï¼š{e}")
                    raise
    
    def _apply_basic_etl_transforms(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‡‰ç”¨åŸºæœ¬çš„ ETL è½‰æ›"""
        # è™•ç†ç¼ºå¤±å€¼
        if df.isna().any().any():
            print("ğŸ”§ è™•ç†ç¼ºå¤±å€¼...")
            df = df.fillna(0)
        
        # ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
        df = df.dropna(how='all')
        
        # ç¢ºä¿æ•¸å€¼æ¬„ä½æ ¼å¼æ­£ç¢º
        numeric_cols = df.select_dtypes(include=['object']).columns
        for col in numeric_cols:
            if df[col].dtype == 'object':
                # å˜—è©¦è½‰æ›ç‚ºæ•¸å€¼
                try:
                    df[col] = pd.to_numeric(df[col], errors='ignore')
                except Exception:
                    pass
        
        return df
    
    def _prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤"""
        # ç¢ºå®šç›®æ¨™æ¬„ä½
        if self.task_type == "binary":
            target_col = "is_attack"
        else:
            target_col = "crlevel"
        
        if target_col not in df.columns:
            raise ValueError(f"âŒ æ‰¾ä¸åˆ°ç›®æ¨™æ¬„ä½ï¼š{target_col}")
        
        # æ’é™¤ç›®æ¨™æ¬„ä½
        feature_cols = [c for c in df.columns if c not in ["is_attack", "crlevel"]]
        
        X = df[feature_cols].copy()
        y = df[target_col].copy()
        
        # è™•ç†ç¼ºå¤±å€¼
        if X.isna().any().any():
            print("âš ï¸ åµæ¸¬åˆ°ç¼ºå¤±å€¼ï¼Œä½¿ç”¨ 0 å¡«å……")
            X = X.fillna(0)
        
        print(f"âœ… ç‰¹å¾µæ•¸é‡ï¼š{len(feature_cols)}")
        print(f"âœ… æ¨™ç±¤åˆ†ä½ˆï¼š\n{y.value_counts()}")
        
        return X, y
    
    def _split_data(
        self, 
        X: pd.DataFrame, 
        y: pd.Series
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†"""
        test_size = self.config.get("test_size", 0.2)
        random_state = self.config.get("random_state", 42)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=test_size, 
            random_state=random_state,
            stratify=y
        )
        
        print(f"âœ… è¨“ç·´é›†ï¼š{len(X_train)} ç­†")
        print(f"âœ… æ¸¬è©¦é›†ï¼š{len(X_test)} ç­†")
        
        return X_train, X_test, y_train, y_test
    
    def _train_models(
        self, 
        X_train: pd.DataFrame, 
        y_train: pd.Series
    ) -> Dict[str, Any]:
        """è¨“ç·´æ¨¡å‹"""
        print(f"\nğŸ¤– é–‹å§‹è¨“ç·´ {self.task_type} æ¨¡å‹...")
        
        # ä½¿ç”¨ ModelBuilder å»ºç«‹å’Œè¨“ç·´æ¨¡å‹
        models = {}
        
        # è¨“ç·´ LightGBM
        print("  âš™ï¸ è¨“ç·´ LightGBM...")
        lgb_model = self.model_builder.build_lightgbm(
            X_train, y_train, 
            task_type=self.task_type
        )
        models["LightGBM"] = lgb_model
        
        # è¨“ç·´ XGBoost
        print("  âš™ï¸ è¨“ç·´ XGBoost...")
        xgb_model = self.model_builder.build_xgboost(
            X_train, y_train,
            task_type=self.task_type
        )
        models["XGBoost"] = xgb_model
        
        # è¨“ç·´ CatBoost
        print("  âš™ï¸ è¨“ç·´ CatBoost...")
        cat_model = self.model_builder.build_catboost(
            X_train, y_train,
            task_type=self.task_type
        )
        models["CatBoost"] = cat_model
        
        print(f"âœ… å®Œæˆè¨“ç·´ {len(models)} å€‹æ¨¡å‹")
        return models
    
    def _evaluate_models(
        self,
        models: Dict[str, Any],
        X_test: pd.DataFrame,
        y_test: pd.Series
    ) -> Dict[str, Dict[str, Any]]:
        """è©•ä¼°æ¨¡å‹"""
        print("\nğŸ“Š è©•ä¼°æ¨¡å‹æ•ˆèƒ½...")
        
        results = {}
        for name, model in models.items():
            print(f"  ğŸ“ˆ è©•ä¼° {name}...")
            y_pred = model.predict(X_test)
            
            # è¨ˆç®—æŒ‡æ¨™
            acc = accuracy_score(y_test, y_pred)
            report = classification_report(y_test, y_pred, output_dict=True)
            cm = confusion_matrix(y_test, y_pred)
            
            results[name] = {
                "accuracy": float(acc),
                "classification_report": report,
                "confusion_matrix": cm.tolist(),
                "predictions": y_pred.tolist()
            }
            
            print(f"    âœ… Accuracy: {acc:.4f}")
        
        return results
    
    def _save_models(
        self, 
        models: Dict[str, Any], 
        out_dir: str
    ) -> Dict[str, str]:
        """å„²å­˜è¨“ç·´å¥½çš„æ¨¡å‹"""
        print("\nğŸ’¾ å„²å­˜æ¨¡å‹...")
        
        saved_paths = {}
        models_dir = os.path.join(out_dir, "models")
        
        for name, model in models.items():
            # æ¨™æº–åŒ–æ¨¡å‹åç¨±
            model_filename = f"{self.task_type}_{name.lower().replace(' ', '_')}.pkl"
            model_path = os.path.join(models_dir, model_filename)
            
            joblib.dump(model, model_path)
            saved_paths[name] = model_path
            print(f"  âœ… {name}: {model_path}")
        
        return saved_paths
    
    def _save_reports(
        self,
        results: Dict[str, Dict[str, Any]],
        out_dir: str
    ) -> str:
        """å„²å­˜è©•ä¼°å ±å‘Š"""
        print("\nğŸ“ å„²å­˜è©•ä¼°å ±å‘Š...")
        
        reports_dir = os.path.join(out_dir, "reports")
        report_path = os.path.join(reports_dir, f"{self.task_type}_evaluation.json")
        
        # æº–å‚™å¯åºåˆ—åŒ–çš„å ±å‘Š
        serializable_results = {}
        for name, result in results.items():
            serializable_results[name] = {
                "accuracy": result["accuracy"],
                "classification_report": result["classification_report"],
                "confusion_matrix": result["confusion_matrix"]
            }
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"  âœ… å ±å‘Š: {report_path}")
        return report_path
    
    def run(self, csv_path: str) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´è¨“ç·´æµç¨‹
        
        Args:
            csv_path: è¨“ç·´è³‡æ–™ CSV è·¯å¾‘
            
        Returns:
            åŒ…å«è¨“ç·´çµæœçš„å­—å…¸
        """
        try:
            # å»ºç«‹è¼¸å‡ºç›®éŒ„
            self.out_dir = self._prepare_artifacts_dir()
            print(f"ğŸ“ è¼¸å‡ºç›®éŒ„ï¼š{self.out_dir}\n")
            
            # 1. è¼‰å…¥è³‡æ–™
            df = self._load_data(csv_path)
            
            # 2. æº–å‚™ç‰¹å¾µ
            X, y = self._prepare_features(df)
            
            # 3. åˆ†å‰²è³‡æ–™
            X_train, X_test, y_train, y_test = self._split_data(X, y)
            
            # 4. è¨“ç·´æ¨¡å‹
            models = self._train_models(X_train, y_train)
            
            # 5. è©•ä¼°æ¨¡å‹
            results = self._evaluate_models(models, X_test, y_test)
            
            # 6. å„²å­˜æ¨¡å‹
            model_paths = self._save_models(models, self.out_dir)
            
            # 7. å„²å­˜å ±å‘Š
            report_path = self._save_reports(results, self.out_dir)
            
            # 8. é›†æˆå­¸ç¿’ï¼ˆå¦‚æœé…ç½®å•Ÿç”¨ï¼‰
            ensemble_result = None
            if self.config.get("ENABLE_ENSEMBLE", False):
                ensemble_result = self._run_ensemble(models, X_train, y_train, X_test, y_test)
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_model_name = max(results.keys(), 
                                 key=lambda k: results[k]["accuracy"])
            best_accuracy = results[best_model_name]["accuracy"]
            
            print("\nâœ¨ è¨“ç·´å®Œæˆï¼")
            print(f"ğŸ† æœ€ä½³æ¨¡å‹ï¼š{best_model_name} (Accuracy: {best_accuracy:.4f})")
            
            return {
                "success": True,
                "output_dir": self.out_dir,
                "models": models,
                "model_paths": model_paths,
                "results": results,
                "report_path": report_path,
                "best_model": best_model_name,
                "best_accuracy": best_accuracy,
                "ensemble_result": ensemble_result
            }
            
        except Exception as e:
            print(f"âŒ è¨“ç·´å¤±æ•—ï¼š{str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e)
            }
    
    def _run_ensemble(self, models: Dict[str, Any], X_train, y_train, X_test, y_test) -> Dict[str, Any]:
        """åŸ·è¡Œé›†æˆå­¸ç¿’"""
        try:
            print("\nğŸ§© é–‹å§‹é›†æˆå­¸ç¿’...")
            
            # æº–å‚™é›†æˆå™¨
            base_estimators = [(name, model) for name, model in models.items()]
            
            combo_opt = ComboOptimizer(
                estimators=base_estimators,
                X_train=X_train,
                y_train=y_train,
                X_valid=X_test,
                y_valid=y_test,
                task_type=self.task_type,
                config=self.config,
                out_dir=self.out_dir
            )
            
            ensemble_result = combo_opt.optimize()
            print("âœ… é›†æˆå­¸ç¿’å®Œæˆï¼")
            return ensemble_result
            
        except Exception as e:
            print(f"âš ï¸ é›†æˆå­¸ç¿’å¤±æ•—ï¼š{e}")
            return {"success": False, "error": str(e)}


# å‘å¾Œç›¸å®¹çš„åˆ¥å
TrainingPipeline = CiscoTrainingPipeline
