# training_pipeline/data_loader.py
import numpy as np
import pandas as pd
from typing import Dict, Tuple


class DataLoader:
    def __init__(self, config: dict):
        self.config = config

    def load_data(self, file_path: str) -> pd.DataFrame:
        """æ™ºèƒ½è¼‰å…¥æ•¸æ“šï¼Œæ”¯æŒè‡ªå‹•æ ¼å¼åµæ¸¬å’Œ ETL é è™•ç†"""
        
                # åµæ¸¬è³‡æ–™æ ¼å¼
        data_format = self._detect_data_format(file_path)
        print(f"ğŸ” åµæ¸¬åˆ°è³‡æ–™æ ¼å¼ï¼š{data_format}")
        
        if data_format == "compressed":
            print("ğŸ“¦ åµæ¸¬åˆ°å£“ç¸®æª”æ¡ˆï¼Œå…ˆè§£å£“ç¸®...")
            extracted_file = self._extract_compressed_file(file_path)
            if extracted_file:
                # éè¿´æª¢æ¸¬è§£å£“å¾Œçš„æª”æ¡ˆæ ¼å¼
                return self.load_data(extracted_file)
            else:
                print("âŒ å£“ç¸®æª”æ¡ˆè§£å£“å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è™•ç†...")
                return self._load_with_fallback_methods(file_path)
        elif data_format == "fortinet_log":
            print("ğŸ”„ åµæ¸¬åˆ° Fortinet æ—¥èªŒæ ¼å¼ï¼ŒåŸ·è¡Œ ETL å‰è™•ç†...")
            return self._process_fortinet_logs(file_path)
        elif data_format == "csv":
            return self._load_with_fallback_methods(file_path)
        else:
            print("âš ï¸ æœªçŸ¥æ ¼å¼ï¼Œå˜—è©¦å„ç¨®è¼‰å…¥æ–¹æ³•...")
            return self._load_with_fallback_methods(file_path)
    
    def _detect_data_format(self, file_path: str) -> str:
        """åµæ¸¬æ•¸æ“šæ ¼å¼ï¼Œæ”¯æ´å£“ç¸®æª”æ¡ˆ"""
        import os
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå£“ç¸®æª”æ¡ˆ
        file_extension = os.path.splitext(file_path)[1].lower()
        if file_extension in ['.gz', '.zip', '.7z', '.rar', '.tar', '.bz2']:
            return "compressed"
        
        try:
            # å˜—è©¦ä»¥æ–‡æœ¬æ–¹å¼è®€å–
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                first_lines = [f.readline().strip() for _ in range(5)]
                first_lines = [line for line in first_lines if line]  # ç§»é™¤ç©ºè¡Œ
            
            # æª¢æŸ¥æ˜¯å¦ç‚º Fortinet æ—¥èªŒæ ¼å¼ (key=value å°)
            fortinet_indicators = 0
            for line in first_lines:
                if not line:
                    continue
                # æª¢æŸ¥ Fortinet æ—¥èªŒçš„ç‰¹å¾µ
                if ('logver=' in line and 'idseq=' in line and 
                    'devid=' in line and 'type=' in line):
                    fortinet_indicators += 1
                # æª¢æŸ¥ key=value æ¨¡å¼
                import re
                kv_pattern = re.compile(r'(\w+)=(".*?"|\'.*?\'|[^"\',\s]+)')
                if len(kv_pattern.findall(line)) > 5:  # è¶…é5å€‹ key=value å°
                    fortinet_indicators += 1
            
            if fortinet_indicators >= 2:
                return "fortinet_log"
            
            # æª¢æŸ¥æ˜¯å¦ç‚º CSV æ ¼å¼
            try:
                pd.read_csv(file_path, nrows=1)
                return "csv"
            except:
                pass
                
            return "unknown"
            
        except Exception as e:
            print(f"æ ¼å¼åµæ¸¬å¤±æ•—: {e}")
            return "unknown"
    
    def _extract_compressed_file(self, file_path: str) -> str | None:
        """è§£å£“ç¸®æª”æ¡ˆä¸¦è¿”å›è§£å£“å¾Œçš„æª”æ¡ˆè·¯å¾‘"""
        import os
        import tempfile
        import zipfile
        import gzip
        import tarfile
        import shutil
        
        file_extension = os.path.splitext(file_path)[1].lower()
        temp_dir = tempfile.mkdtemp()
        
        try:
            if file_extension == '.gz':
                # è™•ç† .gz æª”æ¡ˆ
                output_path = os.path.join(temp_dir, 'extracted_file.txt')
                with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as gz_file:
                    with open(output_path, 'w', encoding='utf-8') as out_file:
                        shutil.copyfileobj(gz_file, out_file)
                print(f"âœ… æˆåŠŸè§£å£“ .gz æª”æ¡ˆåˆ°: {output_path}")
                return output_path
                
            elif file_extension == '.zip':
                # è™•ç† .zip æª”æ¡ˆ
                with zipfile.ZipFile(file_path, 'r') as zip_file:
                    # å–å¾—ç¬¬ä¸€å€‹æª”æ¡ˆ
                    file_list = zip_file.namelist()
                    if not file_list:
                        print("âŒ ZIP æª”æ¡ˆç‚ºç©º")
                        return None
                    
                    first_file = file_list[0]
                    extracted_path = zip_file.extract(first_file, temp_dir)
                    print(f"âœ… æˆåŠŸè§£å£“ .zip æª”æ¡ˆåˆ°: {extracted_path}")
                    return extracted_path
                    
            elif file_extension in ['.tar', '.tar.gz', '.tgz']:
                # è™•ç† .tar ç›¸é—œæª”æ¡ˆ
                with tarfile.open(file_path, 'r:*') as tar_file:
                    # å–å¾—ç¬¬ä¸€å€‹æª”æ¡ˆ
                    members = tar_file.getmembers()
                    if not members:
                        print("âŒ TAR æª”æ¡ˆç‚ºç©º")
                        return None
                    
                    first_member = members[0]
                    if first_member.isfile():
                        tar_file.extract(first_member, temp_dir)
                        extracted_path = os.path.join(temp_dir, first_member.name)
                        print(f"âœ… æˆåŠŸè§£å£“ .tar æª”æ¡ˆåˆ°: {extracted_path}")
                        return extracted_path
                        
            else:
                print(f"âš ï¸ ä¸æ”¯æ´çš„å£“ç¸®æ ¼å¼: {file_extension}")
                return None
                
        except Exception as e:
            print(f"âŒ è§£å£“ç¸®å¤±æ•—: {e}")
            # æ¸…ç†æš«å­˜ç›®éŒ„
            try:
                shutil.rmtree(temp_dir)
            except Exception:
                pass
            return None
    
    def _process_fortinet_logs(self, file_path: str) -> pd.DataFrame:
        """è™•ç† Fortinet åŸå§‹æ—¥èªŒæ ¼å¼"""
        print("ğŸ”„ åµæ¸¬åˆ° Fortinet æ—¥èªŒæ ¼å¼ï¼Œé–‹å§‹ ETL è™•ç†...")
        
        try:
            # å°å…¥ ETL æ¨¡çµ„
            from ..etl_pipeline.log_cleaning import parse_log_line
            import tempfile
            import os
            
            processed_records = []
            
            # è®€å–ä¸¦è§£æåŸå§‹æ—¥èªŒ
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        record = parse_log_line(line.strip())
                        if record:
                            processed_records.append(record)
                        if line_num % 10000 == 0:
                            print(f"ğŸ“ˆ å·²è™•ç† {line_num} è¡Œï¼ŒæˆåŠŸè§£æ {len(processed_records)} æ¢è¨˜éŒ„")
            
            print(f"âœ… ETL è™•ç†å®Œæˆï¼šå…±è§£æ {len(processed_records)} æ¢è¨˜éŒ„")
            
            # è½‰æ›ç‚º DataFrame
            df = pd.DataFrame(processed_records)
            
            # åŸºæœ¬æ•¸æ“šæ¸…ç†å’Œè½‰æ›
            df = self._apply_basic_etl_transforms(df)
            
            print(f"ğŸ“Š æœ€çµ‚æ•¸æ“šå½¢ç‹€: {df.shape}")
            return df
            
        except ImportError as e:
            print(f"âŒ ç„¡æ³•å°å…¥ ETL æ¨¡çµ„: {e}")
            raise RuntimeError("ETL æ¨¡çµ„ä¸å¯ç”¨ï¼Œç„¡æ³•è™•ç† Fortinet æ—¥èªŒæ ¼å¼")
        except Exception as e:
            print(f"âŒ ETL è™•ç†å¤±æ•—: {e}")
            raise RuntimeError(f"ETL è™•ç†å¤±æ•—: {str(e)}")
    
    def _apply_basic_etl_transforms(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‡‰ç”¨åŸºæœ¬çš„ ETL è½‰æ›"""
        try:
            # è™•ç†æ—¥æœŸæ™‚é–“
            if 'date' in df.columns and 'time' in df.columns:
                df['datetime'] = pd.to_datetime(
                    df['date'] + ' ' + df['time'], 
                    errors='coerce'
                )
            
            # è™•ç†æ•¸å€¼æ¬„ä½
            numeric_cols = ['srcport', 'dstport', 'sentpkt', 'rcvdpkt', 'duration', 'crscore']
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            
            # è¨­å®šæ”»æ“Šæ¨™ç±¤
            if 'crscore' in df.columns:
                df['is_attack'] = (pd.to_numeric(df['crscore'], errors='coerce') > 0).astype(int)
            else:
                df['is_attack'] = 0
            
            # è¨­å®šé¢¨éšªç­‰ç´š
            if 'crlevel' not in df.columns:
                df['crlevel'] = 'none'
            
            print("âœ… åŸºæœ¬ ETL è½‰æ›å®Œæˆ")
            return df
            
        except Exception as e:
            print(f"âš ï¸ ETL è½‰æ›éƒ¨åˆ†å¤±æ•—: {e}")
            return df
    
    def _load_csv_with_fallback(self, file_path: str) -> pd.DataFrame:
        """ä½¿ç”¨å®¹éŒ¯æ©Ÿåˆ¶è¼‰å…¥ CSV æ–‡ä»¶"""
        try:
            df = pd.read_csv(file_path)
            print(f"âœ… CSV è¼‰å…¥å®Œæˆï¼š{df.shape[0]} ç­†ï¼Œæ¬„ä½ {df.shape[1]}")
            return df
        except pd.errors.ParserError as e:
            print(f"âš ï¸ CSV æ ¼å¼å•é¡Œï¼š{str(e)}")
            return self._load_with_fallback_methods(file_path)
    
    def _load_with_fallback_methods(self, file_path: str) -> pd.DataFrame:
        """ä½¿ç”¨å¤šç¨®å®¹éŒ¯æ–¹æ³•è¼‰å…¥æ–‡ä»¶"""
        print("ğŸ”„ å˜—è©¦ä½¿ç”¨å®¹éŒ¯æ¨¡å¼é‡æ–°è®€å–...")
        
        # å®¹éŒ¯æ–¹æ³•åˆ—è¡¨
        fallback_methods = [
            {
                "name": "å®¹éŒ¯æ¨¡å¼",
                "params": {
                    "error_bad_lines": False,
                    "warn_bad_lines": True,
                    "on_bad_lines": 'warn'
                }
            },
            {
                "name": "Python å¼•æ“",
                "params": {
                    "sep": None,
                    "engine": 'python',
                    "quoting": 3,
                    "skipinitialspace": True
                }
            }
        ]
        
        for method in fallback_methods:
            try:
                df = pd.read_csv(file_path, **method["params"])
                print(f"âœ… {method['name']}è¼‰å…¥å®Œæˆï¼š{df.shape[0]} ç­†ï¼Œæ¬„ä½ {df.shape[1]}")
                return df
            except Exception as e:
                print(f"âŒ {method['name']}å¤±æ•—: {e}")
                continue
        
        # æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—
        raise RuntimeError(
            f"ç„¡æ³•è¼‰å…¥æ–‡ä»¶ï¼š{file_path}\n"
            f"å·²å˜—è©¦æ‰€æœ‰è¼‰å…¥æ–¹æ³•éƒ½å¤±æ•—ã€‚\n"
            f"è«‹æª¢æŸ¥ï¼š\n"
            f"1. æ–‡ä»¶æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ CSV æ ¼å¼\n"
            f"2. æˆ–è€…æ˜¯å¦ç‚º Fortinet æ—¥èªŒä½†æ ¼å¼ä¸æ­£ç¢º\n"
            f"3. æ–‡ä»¶ç·¨ç¢¼æ˜¯å¦æ­£ç¢ºï¼ˆå»ºè­°ä½¿ç”¨ UTF-8ï¼‰"
        )

    # === æ–°å¢ï¼šå¸¶å ±å‘Šç‰ˆæœ¬ï¼ˆä¸ç ´å£èˆŠä»‹é¢ï¼‰ ===
    @staticmethod
    def prepare_xy_with_report(df: pd.DataFrame, config: dict, task: str) -> Tuple[pd.DataFrame, pd.Series, Dict]:
        """
        èˆ‡èˆŠç‰ˆä¸€è‡´çš„é‚è¼¯ï¼Œä½†é¡å¤–å›å‚³ reportï¼š
        - dropped_by_dropcols
        - dropped_by_type
        - dropped_constant
        - initial_count, final_count, final_cols
        """
        target_col = config["TARGET_COLUMN"]
        drop_cols = set(config.get("DROP_COLUMNS", []))
        if target_col not in df.columns:
            raise KeyError(f"æ‰¾ä¸åˆ°ç›®æ¨™æ¬„ä½ {target_col}")

        report: Dict = {}
        init_cols = list(df.columns)
        report["initial_count"] = len(init_cols)
        report["initial_cols"] = init_cols

        y = df[target_col]
        # 1) ä¸Ÿæ‰ DROP_COLUMNS
        X1 = df.drop(columns=list(drop_cols & set(df.columns)), errors="ignore")
        report["dropped_by_dropcols"] = sorted(list(set(init_cols) - set(X1.columns) - {target_col}))
        report["after_drop_cols"] = list(X1.columns)

        # 2) åƒ…ä¿ç•™ numeric/bool
        keep_cols = []
        for c in X1.columns:
            if c == target_col:
                continue
            dt = X1[c].dtype
            if pd.api.types.is_bool_dtype(dt) or pd.api.types.is_numeric_dtype(dt):
                keep_cols.append(c)
        X2 = X1[keep_cols].copy()
        report["dropped_by_type"] = sorted(list(set(X1.columns) - set(X2.columns)))
        report["after_type_filter"] = list(X2.columns)

        # 3) è£œå€¼ + å‹åˆ¥çµ±ä¸€
        for c in X2.columns:
            if pd.api.types.is_bool_dtype(X2[c].dtype):
                X2[c] = X2[c].fillna(False).astype(np.int8, copy=False)
            else:
                X2[c] = X2[c].fillna(0).astype(np.float32, copy=False)

        # 4) å»å¸¸æ•¸æ¬„ï¼ˆå«å…¨ NaN/å–®ä¸€å€¼ï¼‰
        nunique = X2.nunique(dropna=False)
        const_cols = nunique[nunique <= 1].index.tolist()
        X3 = X2.drop(columns=const_cols, errors="ignore")
        report["dropped_constant"] = const_cols

        report["final_cols"] = list(X3.columns)
        report["final_count"] = int(X3.shape[1])

        # ç¬¬äºŒæ¬¡è¨Šæ¯ï¼šç‰¹å¾µç¯©é¸æ‘˜è¦
        print(f"âœ… ç‰¹å¾µç¯©é¸å®Œæˆï¼šç”± {report['initial_count']} â†’ {report['final_count']} æ¬„")
        print(f"â€¢ ç”± DROP_COLUMNS ç§»é™¤ï¼š{report['dropped_by_dropcols'] or 'ç„¡'}")
        print(f"â€¢ éæ•¸å€¼/å¸ƒæ—è¢«éæ¿¾ï¼š{report['dropped_by_type'] or 'ç„¡'}")
        print(f"â€¢ å¸¸æ•¸/å…¨ç©ºæ¬„ç§»é™¤ï¼š{report['dropped_constant'] or 'ç„¡'}")

        return X3, y, report

    # === èˆŠä»‹é¢ï¼ˆç¶­æŒä¸ç ´å£ï¼‰ ===
    @staticmethod
    def prepare_xy(df: pd.DataFrame, config: dict, task: str):
        X, y, _ = DataLoader.prepare_xy_with_report(df, config, task)
        return X, y
