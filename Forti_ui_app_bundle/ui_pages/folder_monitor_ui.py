import os
import time
import io
import re
import contextlib
import threading
from pathlib import Path

import pandas as pd
import joblib
import streamlit as st
from . import apply_dark_theme  # [ADDED]

try:
    from streamlit_autorefresh import st_autorefresh
except ModuleNotFoundError:  # pragma: no cover - optional dependency
    st_autorefresh = None

from ..etl_pipeliner import run_pipeline
from ..etl_pipeline import log_cleaning as LC
from ..notifier import notify_from_csv


def _rerun() -> None:
    """Trigger a Streamlit rerun across versions."""
    rerun = getattr(st, "rerun", getattr(st, "experimental_rerun", None))
    if rerun is not None:  # pragma: no branch
        rerun()


try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
except Exception:  # pragma: no cover - watchdog may not be installed
    Observer = None
    FileSystemEventHandler = object


class _FileMonitorHandler(FileSystemEventHandler):
    """Watchdog handler that records supported file events."""

    SUPPORTED_EXTS = (
        ".csv",
        ".txt",
        ".log",
        ".csv.gz",
        ".txt.gz",
        ".log.gz",
        ".zip",
    )
    
    # ETL ç”¢ç”Ÿçš„æª”æ¡ˆå¾Œç¶´ï¼Œæ‡‰è©²è¢«éæ¿¾æ‰ï¼ˆæ›´åš´æ ¼çš„éæ¿¾ï¼‰
    ETL_SUFFIXES = (
        "_clean.csv",
        "_preprocessed.csv", 
        "_engineered.csv",
        "_result.csv",
        "_processed.csv",
        "_output.csv",
        "_report.csv",
        "_mapping_report.json",
        # ä¹Ÿéæ¿¾å£“ç¸®ç‰ˆæœ¬
        "_clean.csv.gz",
        "_preprocessed.csv.gz",
        "_engineered.csv.gz",
        "_result.csv.gz",
        "_processed.csv.gz",
        "_output.csv.gz",
        "_report.csv.gz",
    )

    def __init__(self):
        self.events = []
        self.processed_files = set()  # å·²è™•ç†çš„æª”æ¡ˆé›†åˆ
        self.event_signatures = set()  # è¿½è¹¤äº‹ä»¶ç°½ç« é¿å…é‡è¤‡
        self.monitor_thread = None
        self.stop_event = threading.Event()
        self.folder_path = None
        self.use_watchdog = True
        self.log_messages = []

    def _is_etl_generated_file(self, path: str) -> bool:
        """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚º ETL ç”¢ç”Ÿçš„ä¸­é–“æª”æ¡ˆ"""
        path_lower = path.lower()
        # æª¢æŸ¥æª”æ¡ˆåç¨±ä¸­æ˜¯å¦åŒ…å« ETL å¾Œç¶´
        for suffix in self.ETL_SUFFIXES:
            if suffix in path_lower:
                return True
        return False
    
    def _is_already_processed(self, path: str) -> bool:
        """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²è¢«è™•ç†é"""
        # ä½¿ç”¨æª”æ¡ˆè·¯å¾‘å’Œä¿®æ”¹æ™‚é–“ä½œç‚ºå”¯ä¸€æ¨™è­˜
        try:
            stat = os.stat(path)
            file_key = f"{path}_{stat.st_mtime}_{stat.st_size}"
            return file_key in self.processed_files
        except OSError:
            return False
    
    def _mark_as_processed(self, path: str) -> None:
        """æ¨™è¨˜æª”æ¡ˆç‚ºå·²è™•ç†"""
        try:
            stat = os.stat(path)
            file_key = f"{path}_{stat.st_mtime}_{stat.st_size}"
            self.processed_files.add(file_key)
        except OSError:
            pass

    def _should_process_file(self, path: str) -> bool:
        """åˆ¤æ–·æª”æ¡ˆæ˜¯å¦æ‡‰è©²è¢«è™•ç†"""
        # å…ˆéæ¿¾ ETL ç”¢ç”Ÿçš„æª”æ¡ˆï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
        if self._is_etl_generated_file(path):
            return False
        
        # æª¢æŸ¥å‰¯æª”å
        if not path.lower().endswith(self.SUPPORTED_EXTS):
            return False
            
        # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
        if self._is_already_processed(path):
            return False
            
        return True

    def _track(self, event_type: str, path: str) -> None:
        """Record events for supported files that should be processed."""
        # å»ºç«‹äº‹ä»¶ç°½ç« é¿å…çŸ­æ™‚é–“å…§çš„é‡è¤‡äº‹ä»¶
        event_sig = f"{event_type}:{path}"
        if event_sig in self.event_signatures:
            return
            
        if self._should_process_file(path):
            self.events.append((event_type, path))
            self._mark_as_processed(path)
            self.event_signatures.add(event_sig)
            # å®šæœŸæ¸…ç†èˆŠçš„äº‹ä»¶ç°½ç« ï¼ˆä¿ç•™æœ€è¿‘ 1000 å€‹ï¼‰
            if len(self.event_signatures) > 1000:
                self.event_signatures = set(list(self.event_signatures)[-500:])

    def on_created(self, event):  # pragma: no cover - filesystem events
        if not event.is_directory:
            self._track("created", event.src_path)

    def on_modified(self, event):  # pragma: no cover - filesystem events
        if not event.is_directory:
            self._track("modified", event.src_path)

    def start_monitoring(self, folder_path: str, use_watchdog: bool = True):
        """é–‹å§‹ç›£æ§æŒ‡å®šè³‡æ–™å¤¾"""
        self.folder_path = folder_path
        self.use_watchdog = use_watchdog
        self.stop_event.clear()
        
        if self.monitor_thread is None or not self.monitor_thread.is_alive():
            self.monitor_thread = threading.Thread(
                target=self._monitor_loop,
                daemon=True,
                name="FortiFileMonitor"
            )
            self.monitor_thread.start()
            self.log_messages.append(f"âœ… é–‹å§‹ç›£æ§è³‡æ–™å¤¾: {folder_path}")
            return True
        return False

    def stop_monitoring(self):
        """åœæ­¢ç›£æ§"""
        self.stop_event.set()
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5.0)
        self.log_messages.append("â¹ï¸ ç›£æ§å·²åœæ­¢")

    def _monitor_loop(self):
        """æŒçºŒç›£æ§å¾ªç’° - æ”¯æ´é•·æ™‚é–“é‹è¡Œ"""
        sleep_interval = 5  # æ¯5ç§’æª¢æŸ¥ä¸€æ¬¡
        cleanup_counter = 0
        
        self.log_messages.append("ğŸ”„ ç›£æ§å¾ªç’°å•Ÿå‹•")
        
        while not self.stop_event.is_set():
            try:
                if self.use_watchdog:
                    # Watchdog æ¨¡å¼ï¼šè™•ç†ç´¯ç©çš„äº‹ä»¶
                    self._process_watchdog_events()
                else:
                    # è¼ªè©¢æ¨¡å¼ï¼šæƒæè³‡æ–™å¤¾
                    self._inspect_folder()
                
                # æ¯10æ¬¡å¾ªç’°æ¸…ç†ä¸€æ¬¡èˆŠè¨˜éŒ„
                cleanup_counter += 1
                if cleanup_counter >= 10:
                    self._cleanup_old_records()
                    cleanup_counter = 0
                
                # å¯ä¸­æ–·çš„ç¡çœ 
                for _ in range(sleep_interval):
                    if self.stop_event.is_set():
                        break
                    time.sleep(1)
                    
            except Exception as e:
                self.log_messages.append(f"ç›£æ§å¾ªç’°éŒ¯èª¤ï¼š{e}")
                time.sleep(5)  # éŒ¯èª¤æ¢å¾©ç­‰å¾…
        
        self.log_messages.append("ğŸ ç›£æ§å¾ªç’°çµæŸ")

    def _process_watchdog_events(self):
        """è™•ç† Watchdog ç´¯ç©çš„äº‹ä»¶"""
        if not self.events:
            return
        
        new_events = [event for event in self.events if self._should_process_event(event)]
        
        for event_type, path in new_events:
            if self.stop_event.is_set():
                break
                
            try:
                if os.path.exists(path) and self._is_file_stable(path):
                    self._process_single_file(path)
            except Exception as e:
                self.log_messages.append(f"è™•ç†æª”æ¡ˆéŒ¯èª¤ {path}: {e}")

    def _inspect_folder(self):
        """è¼ªè©¢æ¨¡å¼ï¼šæƒæè³‡æ–™å¤¾ä¸­çš„æ–°æª”æ¡ˆ"""
        if not self.folder_path or not os.path.exists(self.folder_path):
            return
        
        try:
            for file_path in Path(self.folder_path).rglob("*"):
                if self.stop_event.is_set():
                    break
                    
                if (file_path.is_file() and 
                    self._should_process_file(str(file_path)) and
                    self._is_file_stable(str(file_path))):
                    
                    self._process_single_file(str(file_path))
                    
        except Exception as e:
            self.log_messages.append(f"è³‡æ–™å¤¾æƒæéŒ¯èª¤ï¼š{e}")

    def _should_process_event(self, event):
        """æª¢æŸ¥äº‹ä»¶æ˜¯å¦æ‡‰è©²è™•ç†"""
        event_type, path = event
        return (os.path.exists(path) and 
                self._should_process_file(path) and
                not self._is_already_processed(path))

    def _is_file_stable(self, path: str, stable_seconds: int = 3) -> bool:
        """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç©©å®šï¼ˆæœªåœ¨å¯«å…¥ä¸­ï¼‰"""
        try:
            return time.time() - os.path.getmtime(path) > stable_seconds
        except OSError:
            return False

    def _process_single_file(self, path: str):
        """è™•ç†å–®å€‹æª”æ¡ˆ - æ·»åŠ åˆ°äº‹ä»¶ä½‡åˆ—ä¾›å¤–éƒ¨è™•ç†"""
        try:
            # æª¢æŸ¥æ˜¯å¦å·²åœ¨äº‹ä»¶ä½‡åˆ—ä¸­
            event_exists = any(event[1] == path for event in self.events if len(event) >= 2)
            if not event_exists:
                # æ·»åŠ åˆ°äº‹ä»¶ä½‡åˆ—ï¼Œè®“å¤–éƒ¨çš„ _process_events å‡½æ•¸è™•ç†
                self.events.append(("file_detected", path, time.time()))
                self.log_messages.append(f"ğŸ“ ç™¼ç¾æ–°æª”æ¡ˆ: {os.path.basename(path)}")
                
                # è§¸ç™¼ Streamlit é‡æ–°æ•´ç†ä»¥è™•ç†æ–°äº‹ä»¶
                if hasattr(st, 'rerun'):
                    # ä½¿ç”¨éé˜»å¡æ–¹å¼é€šçŸ¥æœ‰æ–°äº‹ä»¶
                    pass
                
        except Exception as e:
            self.log_messages.append(f"æª”æ¡ˆåµæ¸¬å¤±æ•— {path}: {e}")
            
    def trigger_event_processing(self):
        """è§¸ç™¼äº‹ä»¶è™•ç† - ä¾›å¤–éƒ¨å‘¼å«"""
        return len(self.events) > 0

    def _cleanup_old_records(self):
        """æ¸…ç†èˆŠè¨˜éŒ„ï¼Œé¿å…è¨˜æ†¶é«”æ´©æ¼"""
        current_time = time.time()
        
        # æ¸…ç† 24 å°æ™‚å‰çš„äº‹ä»¶
        self.events = [
            event for event in self.events 
            if hasattr(event, '__len__') and len(event) >= 3 and 
            current_time - event[2] < 86400
        ] if hasattr(self.events[0] if self.events else None, '__len__') else self.events
        
        # ä¿ç•™æœ€è¿‘ 1000 å€‹è™•ç†è¨˜éŒ„
        if len(self.processed_files) > 1000:
            processed_list = list(self.processed_files)
            self.processed_files = set(processed_list[-1000:])
        
        # ä¿ç•™æœ€è¿‘ 500 å€‹äº‹ä»¶ç°½ç« 
        if len(self.event_signatures) > 500:
            signatures_list = list(self.event_signatures)
            self.event_signatures = set(signatures_list[-500:])
        
        # ä¿ç•™æœ€è¿‘ 100 æ¢æ—¥èªŒè¨Šæ¯
        if len(self.log_messages) > 100:
            self.log_messages = self.log_messages[-100:]

    def get_status(self):
        """å–å¾—ç›£æ§ç‹€æ…‹è³‡è¨Š"""
        is_running = (self.monitor_thread and 
                     self.monitor_thread.is_alive() and 
                     not self.stop_event.is_set())
        
        return {
            'is_running': is_running,
            'folder_path': self.folder_path or 'N/A',
            'use_watchdog': self.use_watchdog,
            'processed_count': len(self.processed_files),
            'pending_events': len(self.events),
            'method': 'Watchdog (å³æ™‚)' if self.use_watchdog else 'è¼ªè©¢ (å®šæœŸ)',
            'last_messages': self.log_messages[-5:] if self.log_messages else []
        }


ANSI_RE = re.compile(r"\x1b\[[0-9;]*[A-Za-z]")


def _log_toast(msg: str) -> None:
    """Append *msg* to log and show a toast if supported."""
    st.session_state.log_lines.append(msg)
    if hasattr(st, "toast"):
        st.toast(msg)
    else:  # pragma: no cover - toast not available
        st.write(msg)


def _run_etl_and_infer(
    path: str, progress_bar, status_placeholder, 
    handler: _FileMonitorHandler = None
) -> None:
    """Run ETL pipeline and model inference on *path*.

    Parameters
    ----------
    path: str
        The path to the file being processed.
    progress_bar: streamlit.delta_generator.DeltaGenerator
        Progress bar widget used for simple progress feedback.
    status_placeholder: streamlit.delta_generator.DeltaGenerator
        Placeholder used to display textual status updates to the user.
    handler: _FileMonitorHandler, optional
        The file monitor handler to mark generated files as processed.
    """
    bin_model = st.session_state.get("binary_model")
    mul_model = st.session_state.get("multi_model")
    if not (bin_model and mul_model):
        status_placeholder.text("Models not uploaded; skipping")
        st.session_state.log_lines.append("Models not uploaded; skipping")
        return

    p = Path(path)
    while p.suffix in {".gz", ".zip"}:
        p = p.with_suffix("")

    ext = p.suffix.lower()
    stem = p.stem.lower()

    clean_csv = path
    do_map = True
    do_fe = True

    if ext in {".txt", ".log"}:
        clean_csv = str(p.with_name(p.stem + "_clean.csv"))

        _log_toast("Running cleaning for raw log")
        LC.clean_logs(quiet=True, paths=[path], clean_csv=clean_csv)
    else:
        clean_csv = path
        if stem.endswith("_engineered"):
            do_map = False
            do_fe = False
        elif stem.endswith("_preprocessed"):
            do_map = False
            do_fe = True

    base = p.with_suffix("")
    pre_csv = clean_csv if not do_map else f"{base}_preprocessed.csv"
    fe_csv = pre_csv if not do_fe else f"{base}_engineered.csv"

    try:
        status_placeholder.text(f"Detected new file: {path}")
        _log_toast(f"Detected new file: {path}")
        buf = io.StringIO()
        status_placeholder.text("Running ETL pipeline...")
        with contextlib.redirect_stdout(buf):
            run_pipeline(
                do_clean=False,
                do_map=do_map,
                do_fe=do_fe,
                clean_out=clean_csv,
                preproc_out=pre_csv,
                fe_out=fe_csv,
            )
        for line in ANSI_RE.sub("", buf.getvalue()).splitlines():
            if line.strip():
                st.session_state.log_lines.append(line.strip())



        # feature engineered data for model inference

        df = pd.read_csv(fe_csv)
        if df.isna().any().any():
            _log_toast("Detected NaNs; filling with 0")
            df.fillna(0, inplace=True)


        # original data retained for notification context
        raw_df = pd.read_csv(clean_csv)
        if raw_df.isna().any().any():
            fill_values = {
                col: 0 if pd.api.types.is_numeric_dtype(raw_df[col]) else ""
                for col in raw_df.columns
            }
            raw_df.fillna(value=fill_values, inplace=True)


        features = [c for c in df.columns if c not in {"is_attack", "crlevel"}]

        bin_clf = bin_model
        bin_features = list(getattr(bin_clf, "feature_names_in_", features))
        missing = [f for f in bin_features if f not in df.columns]
        if missing:
            _log_toast(f"Missing features for binary model: {missing}; filling with 0")
        df_bin = df.reindex(columns=bin_features, fill_value=0)

        status_placeholder.text("Running binary classification...")
        _log_toast("Running binary classification")
        bin_pred = bin_clf.predict(df_bin)
        result = raw_df.copy()

        result["is_attack"] = bin_pred
        result["crlevel"] = 0
        mask = result["is_attack"] == 1
        _log_toast(f"Binary classification found {mask.sum()} attack rows")
        if mask.any():
            mul_clf = mul_model
            mul_features = list(getattr(mul_clf, "feature_names_in_", features))
            missing_mul = [f for f in mul_features if f not in df.columns]
            if missing_mul:
                _log_toast(
                    f"Missing features for multiclass model: {missing_mul}; filling with 0"
                )
            df_mul = df.loc[mask].reindex(columns=mul_features, fill_value=0)

            status_placeholder.text(
                "Running multiclass classification for attack rows..."
            )
            _log_toast("Running multiclass classification for attack rows")
            result.loc[mask, "crlevel"] = mul_clf.predict(df_mul)
        else:
            status_placeholder.text(
                "No attacks detected; skipping multiclass classification"
            )
            _log_toast("No attacks detected; skipping multiclass classification")


        report_path = f"{base}_report.csv"
        result.to_csv(report_path, index=False)
        gen_files = {report_path}
        for f in (clean_csv, pre_csv, fe_csv):
            if f != path:
                gen_files.add(f)
        st.session_state.generated_files.update(gen_files)
        webhook = st.session_state.get("discord_webhook", "")
        gemini_key = st.session_state.get("gemini_key", "")
        line_token = st.session_state.get("line_token", "")
        convergence = st.session_state.get(
            "forti_convergence", {"window_minutes": 10, "group_fields": ["source", "destination"]}
        )


        def _log(msg: str) -> None:
            st.session_state.log_lines.append(msg)
            st.write(msg)

        # æ ¹æ“šç”¨æˆ¶è¨­å®šæ±ºå®šæ˜¯å¦å•Ÿç”¨é€šçŸ¥å’Œè¦–è¦ºåŒ–æ›´æ–°
        enable_notifications = st.session_state.get("enable_notifications", True)
        enable_visualization_sync = st.session_state.get("enable_visualization_sync", True)
        
        if enable_notifications:
            notify_from_csv(
                report_path,
                webhook,
                gemini_key,
                risk_levels={"3", "4"},
                ui_log=_log,
                line_token=line_token,
                convergence=convergence,
            )

        # store counts for visualization
        st.session_state.last_counts = {
            "is_attack": result["is_attack"].value_counts().reindex([0, 1], fill_value=0),
            "crlevel": result["crlevel"].value_counts().reindex([0, 1, 2, 3, 4], fill_value=0),
        }
        st.session_state.last_critical = result[result["crlevel"] >= 4]
        st.session_state.last_report_path = report_path
        
        # è§¸ç™¼è¦–è¦ºåŒ–åŒæ­¥æ›´æ–°
        if enable_visualization_sync:
            st.session_state.visualization_needs_update = True
            st.session_state.visualization_last_update = time.time()

        status_placeholder.text(f"Processed {path} -> {report_path}")
        _log_toast(f"Processed {path} -> {report_path}")
        
        # å¦‚æœæœ‰ handlerï¼Œå°‡ ETL ç”¢ç”Ÿçš„æª”æ¡ˆæ¨™è¨˜ç‚ºå·²è™•ç†ï¼Œé¿å…é‡è¤‡è™•ç†
        if handler:
            for generated_file in gen_files:
                handler._mark_as_processed(generated_file)
        
        for pct in range(0, 101, 20):
            progress_bar.progress(pct)
            time.sleep(0.05)
    except Exception as exc:  # pragma: no cover - processing errors
        _log_toast(f"Processing failed {path}: {exc}")


def _cleanup_generated(hours: int, *, force: bool = False) -> None:
    """Remove generated files older than *hours* or all if *force* is True."""
    now = time.time()
    to_remove = []
    for f in list(st.session_state.get("generated_files", set())):
        try:
            if force or now - os.path.getmtime(f) > hours * 3600:
                os.remove(f)
                to_remove.append(f)
        except OSError:
            to_remove.append(f)
    for f in to_remove:
        st.session_state.generated_files.discard(f)
        st.session_state.log_lines.append(f"Removed {f}")


def _process_events(handler: _FileMonitorHandler, progress_bar, status_placeholder) -> None:
    """Process newly detected files."""
    # ç²å–ä¸Šæ¬¡è™•ç†çš„äº‹ä»¶æ•¸é‡
    last_processed_count = len(st.session_state.get("processed_events", []))
    new_events = handler.events[last_processed_count:]
    
    # å¦‚æœæ²’æœ‰æ–°äº‹ä»¶ï¼Œç›´æ¥è¿”å›
    if not new_events:
        return
    
    processed_in_this_batch = 0
    for event_type, path in new_events:
        # å¤šå±¤æª¢æŸ¥é¿å…é‡è¤‡è™•ç†
        
        # 1. æª¢æŸ¥æ˜¯å¦ç‚º ETL ç”¢ç”Ÿçš„æª”æ¡ˆ
        if handler._is_etl_generated_file(path):
            continue
        
        # 2. æª¢æŸ¥æ˜¯å¦åœ¨ generated_files é›†åˆä¸­
        if path in st.session_state.get("generated_files", set()):
            continue
        
        # 3. æª¢æŸ¥æ˜¯å¦åœ¨ processed_files é›†åˆä¸­
        if path in st.session_state.get("processed_files", set()):
            continue
        
        # 4. æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”ç©©å®šï¼ˆé¿å…è™•ç†æ­£åœ¨å¯«å…¥çš„æª”æ¡ˆï¼‰
        try:
            if not os.path.exists(path):
                continue
            # ç­‰å¾…æª”æ¡ˆç©©å®šï¼ˆæœ€è¿‘ 5 ç§’å…§æœªä¿®æ”¹ï¼‰
            if time.time() - os.path.getmtime(path) < 5:
                continue
        except OSError:
            continue
        
        # è™•ç†æª”æ¡ˆ
        try:
            _run_etl_and_infer(path, progress_bar, status_placeholder, handler)
            st.session_state.setdefault("processed_files", set()).add(path)
            processed_in_this_batch += 1
        except Exception as exc:
            _log_toast(f"Error processing {path}: {exc}")
    
    # æ›´æ–°å·²è™•ç†äº‹ä»¶è¨˜éŒ„
    st.session_state.processed_events = handler.events[:]
    
    # è¨˜éŒ„è™•ç†çµ±è¨ˆ
    if processed_in_this_batch > 0:
        _log_toast(f"Processed {processed_in_this_batch} new file(s) in this batch")

def app() -> None:
    apply_dark_theme()  # [ADDED]
    st.title("Folder Monitor")
    st.info(
        "Select a folder to monitor for CSV/TXT/log files including compressed "
        "formats (.gz, .zip). Files are processed after 5 seconds of inactivity, "
        "and only new data is read to conserve memory."
    )

    if "folder" not in st.session_state:
        st.session_state.folder = os.getcwd()
    previous_folder = st.session_state.folder  # [ADDED]



    if "observer" not in st.session_state:
        st.session_state.observer = None
        st.session_state.handler = None
    st.session_state.setdefault("log_lines", [])
    st.session_state.setdefault("processed_events", [])
    st.session_state.setdefault("generated_files", set())
    st.session_state.setdefault("folder_uploads", set())  # [ADDED]

    # è³‡æ–™å¤¾è¨­å®šå€åŸŸ
    st.subheader("ğŸ“ è³‡æ–™å¤¾ç›£æ§è¨­å®š")
    
    col1, col2, col3 = st.columns([4, 1.5, 1.5])
    with col1:
        # ä½¿ç”¨é¸æ“‡çš„è·¯å¾‘æˆ–é è¨­å€¼
        display_value = (st.session_state.get("selected_folder_path") or 
                        st.session_state.get("folder", os.getcwd()))
        
        # ä½¿ç”¨å”¯ä¸€çš„ key ä¾†å¼·åˆ¶é‡æ–°æ¸²æŸ“
        unique_key = f"folder_input_{hash(display_value)}"
        
        folder_input = st.text_input(
            "ç›£æ§è³‡æ–™å¤¾è·¯å¾‘",
            value=display_value,
            placeholder="è¼¸å…¥è¦ç›£æ§çš„è³‡æ–™å¤¾è·¯å¾‘...",
            help="è«‹è¼¸å…¥æœ‰æ•ˆçš„è³‡æ–™å¤¾è·¯å¾‘é€²è¡Œç›£æ§",
            key=unique_key
        )
        
        # æ¸…é™¤é¸æ“‡ç‹€æ…‹ï¼Œé¿å…é‡è¤‡ä½¿ç”¨
        if "selected_folder_path" in st.session_state:
            del st.session_state.selected_folder_path

    def _use_cwd() -> None:
        current = os.getcwd()
        st.session_state.folder = current
        st.session_state.selected_folder_path = current  # æ–°çš„ç‹€æ…‹è®Šæ•¸
        st.rerun()

    def _browse_folder() -> None:
        # ç°¡åŒ–çš„è³‡æ–™å¤¾ç€è¦½å»ºè­°
        st.session_state.show_folder_examples = True
        _rerun()

    with col2:
        st.markdown("<div style='height: 8px;'></div>", unsafe_allow_html=True)  # å‚ç›´å°é½Š
        st.button(
            "ğŸ“‚ ç€è¦½",
            on_click=_browse_folder,
            help="é¡¯ç¤ºå¸¸ç”¨è³‡æ–™å¤¾è·¯å¾‘ç¯„ä¾‹",
            use_container_width=True,
        )

    with col3:
        st.markdown("<div style='height: 8px;'></div>", unsafe_allow_html=True)  # å‚ç›´å°é½Š
        st.button(
            "ğŸ  ç›®å‰ä½ç½®",
            on_click=_use_cwd,
            help="ä½¿ç”¨ç›®å‰å·¥ä½œç›®éŒ„ä½œç‚ºç›£æ§è³‡æ–™å¤¾",
            use_container_width=True,
        )

    # è™•ç†å¸¸ç”¨è³‡æ–™å¤¾é¸æ“‡
    if st.session_state.get("show_folder_examples", False):
        st.info("ğŸ’¡ **å¸¸ç”¨è³‡æ–™å¤¾ç¯„ä¾‹ - é»æ“Šé¸æ“‡ï¼š**")
        example_cols = st.columns(3)
        
        common_folders = [
            ("ğŸ“ æ¡Œé¢", os.path.expanduser("~/Desktop")),
            ("ğŸ“ æ–‡ä»¶", os.path.expanduser("~/Documents")),
            ("ğŸ“ ä¸‹è¼‰", os.path.expanduser("~/Downloads")),
        ]
        
        for i, (name, path) in enumerate(common_folders):
            with example_cols[i % 3]:
                # å‰µå»ºå®‰å…¨çš„æŒ‰éˆ•éµå€¼
                safe_path = path.replace('/', '_').replace('\\', '_')
                button_key = f"folder_example_{i}_{safe_path}"
                if st.button(name, key=button_key, use_container_width=True):
                    st.session_state.show_folder_examples = False
                    # åªæ›´æ–°å…§éƒ¨ç‹€æ…‹ï¼Œä¸è§¸ç¢° widget çš„ session_state
                    st.session_state.folder = path
                    st.session_state.selected_folder_path = path  # æ–°çš„ç‹€æ…‹è®Šæ•¸
                    st.success(f"âœ… å·²é¸æ“‡è³‡æ–™å¤¾ï¼š{path}")
                    st.rerun()

    # ä½¿ç”¨ç”¨æˆ¶è¼¸å…¥çš„è·¯å¾‘æˆ–é è¨­å€¼
    folder_candidate = folder_input.strip() if folder_input else ""
    if folder_candidate:
        folder_path = Path(folder_candidate).expanduser()
        # æ›´æ–° session state ä»¥ä¿æŒåŒæ­¥
        st.session_state.folder = folder_candidate
    else:
        folder_path = Path(st.session_state.get("folder", os.getcwd()))

    folder_error = None
    try:
        folder_path.mkdir(parents=True, exist_ok=True)
        folder_path = folder_path.resolve()
    except OSError as exc:
        folder_error = str(exc)
        folder_valid = False
    else:
        folder_valid = folder_path.is_dir()

    if folder_valid:
        resolved_folder = str(folder_path)
        if resolved_folder != previous_folder:
            st.session_state.folder_uploads = set()
        st.session_state.folder = resolved_folder
        st.success(f"âœ… ç›£æ§è·¯å¾‘ï¼š{folder_path}")
    else:
        if folder_candidate:
            if folder_error:
                st.error(f"âŒ ç„¡æ³•ä½¿ç”¨è³‡æ–™å¤¾ï¼š{folder_error}")
            else:
                st.error("âŒ æä¾›çš„è·¯å¾‘ä¸æ˜¯æœ‰æ•ˆçš„è³‡æ–™å¤¾")
        st.warning("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„è³‡æ–™å¤¾è·¯å¾‘")

    # æª”æ¡ˆä¸Šå‚³å€åŸŸ
    st.subheader("ğŸ“¤ æª”æ¡ˆä¸Šå‚³")
    uploaded_logs = st.file_uploader(
        "ä¸Šå‚³æ—¥èªŒæª”æ¡ˆæˆ–å£“ç¸®åŒ…åˆ°ç›£æ§è³‡æ–™å¤¾",
        type=["csv", "txt", "log", "gz", "zip"],
        accept_multiple_files=True,
        help="æª”æ¡ˆå°‡ä¿å­˜åˆ°ç›£æ§è³‡æ–™å¤¾ä¸­ä¸¦è‡ªå‹•è™•ç†",
        key="folder_monitor_upload",
    )

    if uploaded_logs:  # [ADDED]
        if folder_valid:  # [ADDED]
            processed_uploads = st.session_state.get("folder_uploads", set())  # [ADDED]
            saved_count = 0  # [ADDED]
            progress_bar_upload = st.progress(0)
            status_placeholder_upload = st.empty()
            
            for uploaded in uploaded_logs:  # [ADDED]
                signature = (uploaded.name, uploaded.size)  # [ADDED]
                if signature in processed_uploads:  # [ADDED]
                    continue  # [ADDED]
                destination = folder_path / uploaded.name  # [ADDED]
                with open(destination, "wb") as dest_file:  # [ADDED]
                    dest_file.write(uploaded.getbuffer())  # [ADDED]
                processed_uploads.add(signature)  # [ADDED]
                saved_count += 1  # [ADDED]
                _log_toast(f"Uploaded {destination}")  # [ADDED]
                
                # ç«‹å³è™•ç†ä¸Šå‚³çš„æª”æ¡ˆï¼Œä¸ç­‰å¾… watchdog äº‹ä»¶
                has_models = (st.session_state.get("binary_model") and
                              st.session_state.get("multi_model"))
                if has_models:
                    try:
                        status_placeholder_upload.text(
                            f"Processing uploaded file: {uploaded.name}")
                        # å˜—è©¦ç²å–ç•¶å‰çš„ handlerï¼Œç”¨æ–¼æ¨™è¨˜ç”¢ç”Ÿæª”æ¡ˆ
                        current_handler = st.session_state.get("handler")
                        _run_etl_and_infer(str(destination), 
                                           progress_bar_upload, 
                                           status_placeholder_upload,
                                           current_handler)
                        processed_files = st.session_state.setdefault(
                            "processed_files", set())
                        processed_files.add(str(destination))
                        _log_toast(
                            f"Immediately processed uploaded file: {uploaded.name}")
                    except Exception as exc:
                        _log_toast(
                            f"Failed to process uploaded file {uploaded.name}: {exc}")
                else:
                    _log_toast(
                        f"Models not loaded, {uploaded.name} will be processed "
                        "when monitoring starts")
                    
            st.session_state.folder_uploads = processed_uploads  # [ADDED]
            if saved_count:  # [ADDED]
                st.success(f"Saved and processed {saved_count} file(s) to {folder_path}")  # [ADDED]
            else:  # [ADDED]
                st.info("Uploaded files are already available in the monitored folder.")  # [ADDED]
        else:  # [ADDED]
            st.error("Enter a valid folder path before uploading files.")  # [ADDED]

    # æ¨¡å‹è¼‰å…¥å€åŸŸ
    st.subheader("âš™ï¸ æ©Ÿå™¨å­¸ç¿’æ¨¡å‹")
    
    model_cols = st.columns(2)
    
    with model_cols[0]:
        bin_upload = st.file_uploader(
            "ğŸ“Š äºŒå…ƒåˆ†é¡æ¨¡å‹",
            type=["pkl", "joblib"],
            help="ç”¨æ–¼åˆ¤æ–·æ˜¯å¦ç‚ºæ”»æ“Šçš„äºŒå…ƒåˆ†é¡æ¨¡å‹ (æœ€å¤§æª”æ¡ˆå¤§å°ï¼š2GB)",
            key="binary_model_upload",
        )
        if bin_upload is not None:
            try:
                st.session_state.binary_model = joblib.load(bin_upload)
                st.success("âœ… äºŒå…ƒåˆ†é¡æ¨¡å‹å·²è¼‰å…¥")
            except Exception:
                st.error("âŒ äºŒå…ƒåˆ†é¡æ¨¡å‹è¼‰å…¥å¤±æ•—")
                st.session_state.log_lines.append("Failed to load binary model")

    with model_cols[1]:
        mul_upload = st.file_uploader(
            "ğŸ¯ å¤šå…ƒåˆ†é¡æ¨¡å‹",
            type=["pkl", "joblib"],
            help="ç”¨æ–¼åˆ¤æ–·æ”»æ“Šåš´é‡ç¨‹åº¦çš„å¤šå…ƒåˆ†é¡æ¨¡å‹ (æœ€å¤§æª”æ¡ˆå¤§å°ï¼š2GB)",
            key="multi_model_upload",
        )
        if mul_upload is not None:
            try:
                st.session_state.multi_model = joblib.load(mul_upload)
                st.success("âœ… å¤šå…ƒåˆ†é¡æ¨¡å‹å·²è¼‰å…¥")
            except Exception:
                st.error("âŒ å¤šå…ƒåˆ†é¡æ¨¡å‹è¼‰å…¥å¤±æ•—")
                st.session_state.log_lines.append("Failed to load multiclass model")

    # é¡¯ç¤ºæ¨¡å‹ç‹€æ…‹
    if st.session_state.get("binary_model") and st.session_state.get("multi_model"):
        st.info("ğŸŸ¢ **æ‰€æœ‰æ¨¡å‹å·²å°±ç·’ï¼Œå¯ä»¥é–‹å§‹ç›£æ§è™•ç†**")
    elif st.session_state.get("binary_model") or st.session_state.get("multi_model"):
        st.warning("ğŸŸ¡ **éƒ¨åˆ†æ¨¡å‹å·²è¼‰å…¥ï¼Œéœ€è¦å…©å€‹æ¨¡å‹æ‰èƒ½å®Œæ•´è™•ç†**")
    else:
        st.warning("ğŸ”´ **è«‹å…ˆä¸Šå‚³å…©å€‹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹æ‰èƒ½é€²è¡Œç›£æ§è™•ç†**")

    # è¨­å®šå€åŸŸ
    st.subheader("âš™ï¸ ç›£æ§è¨­å®š")
    
    settings_cols = st.columns([2, 1])
    with settings_cols[0]:
        retention = st.number_input(
            "è‡ªå‹•æ¸…ç†æª”æ¡ˆ (å°æ™‚ï¼Œ0=é—œé–‰)",
            min_value=0,
            value=0,
            step=1,
            key="cleanup_hours",
            help="è‡ªå‹•åˆªé™¤è¶…éæŒ‡å®šå°æ™‚æ•¸çš„ç”Ÿæˆæª”æ¡ˆ",
        )
    
    with settings_cols[1]:
        st.markdown("<div style='height: 8px;'></div>", unsafe_allow_html=True)
        if st.button("ğŸ—‘ï¸ ç«‹å³æ¸…ç†", use_container_width=True, 
                     help="ç«‹å³æ¸…ç†æ‰€æœ‰ç”Ÿæˆçš„æª”æ¡ˆ"):
            _cleanup_generated(0, force=True)
            st.success("å·²æ¸…ç†æ‰€æœ‰ç”Ÿæˆæª”æ¡ˆ")

    # æ§åˆ¶æŒ‰éˆ•å€åŸŸ
    st.subheader("ğŸ›ï¸ ç›£æ§æ§åˆ¶")
    action_cols = st.columns(2)

    if Observer is None:
        st.error("âŒ **watchdog å¥—ä»¶æœªå®‰è£**ï¼Œç„¡æ³•ä½¿ç”¨æª”æ¡ˆç›£æ§åŠŸèƒ½")
        return

    folder = st.session_state.folder
    is_monitoring = st.session_state.observer is not None
    start_disabled = is_monitoring or not folder_valid
    stop_disabled = not is_monitoring

    status_placeholder = st.empty()
    start_help = ("è«‹å…ˆè¼¸å…¥æœ‰æ•ˆçš„è³‡æ–™å¤¾è·¯å¾‘" if not folder_valid 
                  else "ç›£æ§å·²å•Ÿå‹•" if is_monitoring else "é–‹å§‹ç›£æ§æŒ‡å®šè³‡æ–™å¤¾")

    with action_cols[0]:
        start_button_text = "ğŸ”´ ç›£æ§é‹è¡Œä¸­" if is_monitoring else "â–¶ï¸ é–‹å§‹ç›£æ§"
        if st.button(
            start_button_text,
            disabled=start_disabled,
            help=start_help,
            use_container_width=True,
            type="secondary" if is_monitoring else "primary",
        ):
            # å»ºç«‹å¢å¼·ç‰ˆçš„è™•ç†å™¨
            handler = _FileMonitorHandler()
            
            # å•Ÿå‹• Watchdog è§€å¯Ÿå™¨
            observer = Observer()
            observer.schedule(handler, folder, recursive=False)
            observer.start()
            
            # å•Ÿå‹•æŒçºŒç›£æ§å¾ªç’°
            use_watchdog = Observer is not None
            handler.start_monitoring(folder, use_watchdog)
            
            # å„²å­˜åˆ° session state
            st.session_state.observer = observer
            st.session_state.handler = handler
            status_placeholder.success(f"âœ… å·²é–‹å§‹ç›£æ§ï¼š{folder}")
            _log_toast(f"Enhanced monitoring started on {folder}")

    with action_cols[1]:
        stop_button_text = "â¹ï¸ åœæ­¢ç›£æ§"
        if st.button(stop_button_text, disabled=stop_disabled, 
                     use_container_width=True, type="secondary"):
            # åœæ­¢æŒçºŒç›£æ§
            handler = st.session_state.get("handler")
            if handler:
                handler.stop_monitoring()
            
            # åœæ­¢ Watchdog è§€å¯Ÿå™¨
            observer = st.session_state.observer
            if observer is not None:
                observer.stop()
                observer.join()
                
            # æ¸…ç† session state
            st.session_state.observer = None
            st.session_state.handler = None
            status_placeholder.info("â¹ï¸ ç›£æ§å·²åœæ­¢")
            _log_toast("Enhanced monitoring stopped")

    # è™•ç†é€²åº¦å’Œç‹€æ…‹é¡¯ç¤º
    st.subheader("ğŸ“Š è™•ç†ç‹€æ…‹")
    
    # é¡¯ç¤ºè©³ç´°ç›£æ§ç‹€æ…‹
    handler = st.session_state.get("handler")
    if handler:
        status_info = handler.get_status()
        
        # ç‹€æ…‹æ¦‚è¦½
        status_cols = st.columns([2, 2, 2])
        with status_cols[0]:
            status_emoji = "ğŸŸ¢" if status_info['is_running'] else "ğŸ”´"
            status_text = "ç›£æ§ä¸­" if status_info['is_running'] else "å·²åœæ­¢"
            st.metric("ç›£æ§ç‹€æ…‹", f"{status_emoji} {status_text}")
            
        with status_cols[1]:
            st.metric("å·²è™•ç†æª”æ¡ˆ", f"{status_info['processed_count']} å€‹")
            
        with status_cols[2]:
            st.metric("å¾…è™•ç†äº‹ä»¶", f"{status_info['pending_events']} å€‹")
        
        # è©³ç´°ç‹€æ…‹è¡¨æ ¼
        st.markdown("**ğŸ“‹ è©³ç´°ç›£æ§è³‡è¨Š**")
        status_data = {
            "é …ç›®": ["ç›£æ§è³‡æ–™å¤¾", "ç›£æ§æ–¹å¼", "é‹è¡Œç‹€æ…‹", "å·²è™•ç†æª”æ¡ˆ", "å¾…è™•ç†äº‹ä»¶"],
            "å…§å®¹": [
                status_info['folder_path'],
                status_info['method'],
                "ğŸŸ¢ é‹è¡Œä¸­" if status_info['is_running'] else "ğŸ”´ å·²åœæ­¢",
                f"{status_info['processed_count']} å€‹æª”æ¡ˆ",
                f"{status_info['pending_events']} å€‹äº‹ä»¶"
            ]
        }
        st.table(pd.DataFrame(status_data))
        
        # æœ€è¿‘æ´»å‹•è¨Šæ¯
        if status_info['last_messages']:
            st.markdown("**ğŸ“¢ æœ€è¿‘æ´»å‹•**")
            for msg in status_info['last_messages']:
                st.text(f"â€¢ {msg}")
    
    progress_bar = st.progress(0)

    if st.session_state.observer is not None:
        _process_events(st.session_state.handler, progress_bar, 
                        status_placeholder)
        _cleanup_generated(retention)

    # å ±å‘Šçµæœé¡¯ç¤º
    report_path = st.session_state.get("last_report_path")
    if report_path:
        st.success(f"ğŸ“‹ **å ±å‘Šå·²ç”Ÿæˆ**ï¼š{report_path}")
        
        # é¡¯ç¤ºç°¡æ˜“çµ±è¨ˆé è¦½
        counts = st.session_state.get("last_counts")
        if counts:
            preview_cols = st.columns(3)
            with preview_cols[0]:
                total = int(counts["is_attack"].sum())
                st.metric("ç¸½äº‹ä»¶æ•¸", f"{total:,}")
            with preview_cols[1]:
                attacks = int(counts["is_attack"].get(1, 0))
                attack_percentage = (attacks/total*100) if total > 0 else 0
                st.metric("æ”»æ“Šäº‹ä»¶", f"{attacks:,}", 
                         delta=f"{attack_percentage:.1f}% æ”»æ“Šç‡" if total > 0 else "0% æ”»æ“Šç‡",
                         help=f"åœ¨ {total:,} å€‹ç¸½äº‹ä»¶ä¸­ï¼Œæœ‰ {attacks:,} å€‹è¢«è­˜åˆ¥ç‚ºæ”»æ“Šäº‹ä»¶ï¼Œæ”»æ“Šç‡ç‚º {attack_percentage:.1f}%")
            with preview_cols[2]:
                cr_counts = counts.get("crlevel")
                if cr_counts is not None and not cr_counts.empty:
                    high_risk = int(cr_counts.loc[cr_counts.index >= 3].sum())
                    st.metric("é«˜é¢¨éšªäº‹ä»¶", f"{high_risk:,}")
                else:
                    st.metric("é«˜é¢¨éšªäº‹ä»¶", "0")
        
        # æä¾›ç›´æ¥æŸ¥çœ‹è¦–è¦ºåŒ–çš„æç¤º
        st.info("ğŸ’¡ **è«‹å‰å¾€ 'Visualization' é é¢æŸ¥çœ‹è©³ç´°åœ–è¡¨å’Œåˆ†æçµæœ**")
        
        # é¡¯ç¤ºå ±å‘Šæª”æ¡ˆä½ç½®
        st.caption(f"ğŸ“ å ±å‘Šæª”æ¡ˆä½ç½®ï¼š{report_path}")

    # æ—¥èªŒå€åŸŸ
    st.subheader("ğŸ“ è™•ç†æ—¥èªŒ")
    log_container = st.container()
    with log_container:
        if st.session_state.log_lines:
            # é¡¯ç¤ºæœ€æ–°çš„æ—¥èªŒæ¢ç›®
            recent_logs = st.session_state.log_lines[-10:]  # æœ€æ–°10æ¢
            for log_line in recent_logs:
                st.text(log_line)
            
            if len(st.session_state.log_lines) > 10:
                if st.button("ğŸ“œ é¡¯ç¤ºå®Œæ•´æ—¥èªŒ"):
                    st.text_area("å®Œæ•´æ—¥èªŒ", 
                               "\n".join(st.session_state.log_lines), 
                               height=200)
        else:
            st.info("æš«ç„¡è™•ç†æ—¥èªŒ")

    # æŒçºŒç›£æ§å’Œè‡ªå‹•é‡æ–°æ•´ç†
    if st.session_state.observer is not None:
        handler = st.session_state.handler
        
        # æª¢æŸ¥ç›£æ§ç‹€æ…‹å’Œæ–°äº‹ä»¶
        if handler:
            status_info = handler.get_status()
            is_monitoring = status_info.get('is_running', False)
            has_pending_events = status_info.get('pending_events', 0) > 0
            
            # é¡¯ç¤ºå³æ™‚ç›£æ§ç‹€æ…‹
            if is_monitoring:
                st.info(f"ğŸ”„ æŒçºŒç›£æ§ä¸­... å¾…è™•ç†äº‹ä»¶: {status_info.get('pending_events', 0)} å€‹")
            
            # å¦‚æœæ­£åœ¨ç›£æ§ï¼Œä½¿ç”¨çŸ­é–“éš”è‡ªå‹•é‡æ–°æ•´ç†
            if is_monitoring or has_pending_events:
                if st_autorefresh is not None:
                    st_autorefresh(interval=2000, key="continuous_monitor_refresh")
                else:  # pragma: no cover - fallback when autorefresh missing
                    time.sleep(1)
                    _rerun()
            else:
                # ç›£æ§åœæ­¢æ™‚ï¼Œä½¿ç”¨è¼ƒé•·é–“éš”æª¢æŸ¥
                if st_autorefresh is not None:
                    st_autorefresh(interval=5000, key="monitor_idle_check")

