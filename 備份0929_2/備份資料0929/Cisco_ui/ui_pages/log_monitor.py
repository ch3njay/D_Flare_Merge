"""Cisco ç‰ˆæœ¬çš„ Log ç›£è½èˆ‡è‡ªå‹•æ¸…æ´—æ¨¡çµ„ï¼ˆStreamlit ç‰ˆï¼‰ã€‚

æ­¤æ¨¡çµ„å°‡åŸæœ¬é›†ä¸­æ–¼ PyQt5 ä»‹é¢çš„ç¨‹å¼ç¢¼é‡æ§‹ç‚º Streamlit ç›¸å®¹çš„å¯«æ³•ï¼Œ
å®Œæ•´ä¿ç•™è‡ªå‹•ç›£è½ã€è³‡æ–™æ¸…æ´—ã€æ¨¡å‹æ¨è«–èˆ‡é€šçŸ¥ä¸²æ¥çš„åŠŸèƒ½ï¼Œä¸¦è£œå¼·
åŸ·è¡Œç·’æ§ç®¡èˆ‡æ—¥èªŒå‘ˆç¾ï¼Œæ–¹ä¾¿åœ¨ç¶²é åŒ–ä»‹é¢ä¸­ç¶­è­·èˆ‡æ“´å……ã€‚
"""
from __future__ import annotations

import html
import os
import shutil
import subprocess
import sys
import tempfile
import threading
import time
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import pandas as pd
import streamlit as st

try:  # Prefer package-relative imports when available
    from ..training_pipeline.config import PipelineConfig
    from ..training_pipeline.trainer import execute_pipeline
    from ..notifier import notification_pipeline
    from ..utils_labels import append_log, load_json, save_json
except (ImportError, ValueError):  # Support running within the package directory directly
    from training_pipeline.config import PipelineConfig  # type: ignore[no-redef]
    from training_pipeline.trainer import execute_pipeline  # type: ignore[no-redef]
    from notifier import notification_pipeline  # type: ignore[no-redef]
    from utils_labels import append_log, load_json, save_json  # type: ignore[no-redef]

# è¨­å®šæª”è·¯å¾‘èˆ‡é è¨­å€¼å®šç¾©
LOG_SETTINGS_FILE = "logfetcher_settings.json"
NOTIFIER_SETTINGS_FILE = "notifier_settings.txt"
DEFAULT_LOG_SETTINGS = {
    "save_dir": "",
    "binary_model_path": "",
    "model_path": "",
    "clean_csv_dir": "",
}
DEFAULT_NOTIFIER_SETTINGS = {
    "gemini_api_key": "",
    "line_channel_secret": "",
    "line_channel_access_token": "",
    "line_webhook_url": "",
    "discord_webhook_url": "",
}

PATH_BROWSER_ROOT = Path(tempfile.gettempdir()) / "df_cisco_paths"


class LogMonitor:
    """è² è²¬ç¶­è­·è³‡æ–™å¤¾ç›£æ§ç‹€æ…‹èˆ‡è‡ªå‹•æ¸…æ´—æµç¨‹çš„æ ¸å¿ƒç‰©ä»¶ã€‚"""

    def __init__(self) -> None:
        self.settings: Dict[str, str] = load_json(LOG_SETTINGS_FILE, DEFAULT_LOG_SETTINGS)
        self.log_messages: List[str] = []
        self.listening = False
        self.stop_event = threading.Event()
        self.monitor_thread: Optional[threading.Thread] = None
        self.socket_process: Optional[subprocess.Popen[str]] = None
        self.processed_files: Set[Tuple[str, int, float]] = set()
        self.notified_multiclass_files: Set[str] = set()
        self.last_file_checked = ""
        self.last_file_size = 0
        self.file_stable_count = 0
        self.last_processed_file = ""
        self.cleaning_lock = threading.Lock()
        self.latest_result: Optional[Dict[str, object]] = None
        self.paused = False
        self._last_folder_error: Optional[str] = None

    # ==== ç‹€æ…‹ç®¡ç† ====
    def update_settings(self, **kwargs: str) -> None:
        """æ›´æ–°ç›£æ§è¨­å®šä¸¦ç«‹å³å¯«å…¥è¨­å®šæª”ã€‚"""
        self.settings.update({k: v.strip() for k, v in kwargs.items()})
        save_json(LOG_SETTINGS_FILE, self.settings)
        append_log(self.log_messages, "âœ… ç›£æ§è¨­å®šå·²å„²å­˜")

    def start_listening(self) -> None:
        """å•Ÿå‹•è³‡æ–™å¤¾ç›£æ§èˆ‡èƒŒæ™¯æƒæåŸ·è¡Œç·’ã€‚"""
        if self.listening:
            append_log(self.log_messages, "âš ï¸ ç›£è½å·²åœ¨åŸ·è¡Œä¸­")
            return

        required = {
            "log å„²å­˜è³‡æ–™å¤¾": self.settings.get("save_dir", ""),
            "äºŒå…ƒæ¨¡å‹è·¯å¾‘": self.settings.get("binary_model_path", ""),
            "å¤šå…ƒæ¨¡å‹è·¯å¾‘": self.settings.get("model_path", ""),
            "æ¸…æ´—è¼¸å‡ºè³‡æ–™å¤¾": self.settings.get("clean_csv_dir", ""),
        }
        missing = [label for label, value in required.items() if not value]
        if missing:
            append_log(self.log_messages, f"â— ç¼ºå°‘å¿…è¦è¨­å®šï¼š{'ã€'.join(missing)}")
            return

        save_dir = required["log å„²å­˜è³‡æ–™å¤¾"]
        if not os.path.exists(save_dir):
            append_log(self.log_messages, f"âŒ ç›£æ§è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼š{save_dir}")
            return
        if not os.access(save_dir, os.W_OK):
            append_log(self.log_messages, f"âŒ ç„¡æ³•å¯«å…¥ç›£æ§è³‡æ–™å¤¾ï¼š{save_dir}")
            return

        self.stop_event.clear()
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        self.listening = True
        append_log(self.log_messages, f"ğŸŸ¢ å·²å•Ÿå‹•ç›£è½ï¼Œè·¯å¾‘ï¼š{save_dir}")
        self._start_socket_process(save_dir)

    def _start_socket_process(self, save_dir: str) -> None:
        """è‹¥å°ˆå±¬ socket è…³æœ¬å­˜åœ¨å‰‡åŒæ­¥å•Ÿå‹•å­ç¨‹åºã€‚"""
        script_path = os.path.join(os.getcwd(), "socket_5.py")
        if not os.path.exists(script_path):
            append_log(self.log_messages, "â„¹ï¸ æ‰¾ä¸åˆ° socket_5.pyï¼Œåƒ…å•Ÿå‹•è³‡æ–™å¤¾ç›£æ§ã€‚")
            return
        try:
            self.socket_process = subprocess.Popen(
                [sys.executable, script_path, save_dir],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
            )
            append_log(self.log_messages, "ğŸš€ å·²å•Ÿå‹• socket_5.py å­ç¨‹åº")
            threading.Thread(target=self._capture_socket_output, daemon=True).start()
        except Exception as exc:
            append_log(self.log_messages, f"âŒ ç„¡æ³•å•Ÿå‹• socket å­ç¨‹åºï¼š{exc}")
            self.socket_process = None

    def _capture_socket_output(self) -> None:
        """æŒçºŒè®€å– socket å­ç¨‹åºè¼¸å‡ºä¸¦è¨˜éŒ„æ–¼æ—¥èªŒã€‚"""
        if not self.socket_process or not self.socket_process.stdout:
            return
        for line in self.socket_process.stdout:
            line = line.strip()
            if line:
                append_log(self.log_messages, f"[socket] {line}")
        append_log(self.log_messages, "ğŸ›‘ socket å­ç¨‹åºå·²çµæŸ")

    def stop_listening(self) -> None:
        """åœæ­¢ç›£æ§èˆ‡æ‰€æœ‰èƒŒæ™¯å·¥ä½œã€‚"""
        if not self.listening:
            append_log(self.log_messages, "âš ï¸ å°šæœªå•Ÿå‹•ç›£è½")
            return
        self.stop_event.set()
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=3)
        self.monitor_thread = None
        self.listening = False
        append_log(self.log_messages, "â›” å·²åœæ­¢ç›£è½")

        if self.socket_process:
            try:
                self.socket_process.terminate()
                self.socket_process.wait(timeout=3)
                append_log(self.log_messages, "ğŸ›‘ socket å­ç¨‹åºå·²çµ‚æ­¢")
            except Exception as exc:
                append_log(self.log_messages, f"âš ï¸ socket å­ç¨‹åºçµ‚æ­¢å¤±æ•—ï¼š{exc}")
            finally:
                self.socket_process = None

    # ==== è³‡æ–™å¤¾æƒæé‚è¼¯ ====
    def _monitor_loop(self) -> None:
        """èƒŒæ™¯åŸ·è¡Œç·’ï¼šæ¯ 5 ç§’æƒæä¸€æ¬¡è³‡æ–™å¤¾ã€‚"""
        while not self.stop_event.is_set():
            if not self.paused:
                self._inspect_folder()
            time.sleep(5)

    def _inspect_folder(self, manual: bool = False) -> None:
        """æƒæè³‡æ–™å¤¾ï¼Œè‹¥æ‰¾åˆ°ç©©å®šçš„æœ€æ–°æª”æ¡ˆä¾¿å•Ÿå‹•è‡ªå‹•æ¸…æ´—ã€‚"""
        folder = self.settings.get("save_dir", "").strip()
        if not folder:
            if manual:
                append_log(self.log_messages, "âš ï¸ è«‹å…ˆè¨­å®š log å„²å­˜è³‡æ–™å¤¾")
            return
        if not os.path.isdir(folder):
            if manual or self._last_folder_error != folder:
                append_log(self.log_messages, f"â— ç›£æ§è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼š{folder}")
                self._last_folder_error = folder
            return
        self._last_folder_error = None

        try:
            files = [
                f
                for f in os.listdir(folder)
                if f.endswith(".csv") and f.startswith("asa_logs_") and "_result" not in f
            ]
        except Exception as exc:
            append_log(self.log_messages, f"âŒ ç„¡æ³•è®€å–è³‡æ–™å¤¾ï¼š{exc}")
            return

        if not files:
            if manual:
                append_log(self.log_messages, "â„¹ï¸ ç›®å‰è³‡æ–™å¤¾æ²’æœ‰æ–°çš„ log æª”æ¡ˆ")
            return

        files.sort(key=lambda name: os.path.getmtime(os.path.join(folder, name)), reverse=True)
        latest_file = os.path.join(folder, files[0])
        current_size = os.path.getsize(latest_file) if os.path.exists(latest_file) else 0
        now = time.time()

        # ç§»é™¤è¶…éä¸€å°æ™‚çš„ç´€éŒ„
        self.processed_files = {
            item for item in self.processed_files if now - item[2] < 3600
        }
        if any(item[0] == latest_file and item[1] == current_size for item in self.processed_files):
            return

        if latest_file != self.last_file_checked:
            self.last_file_checked = latest_file
            self.last_file_size = current_size
            self.file_stable_count = 1
            append_log(self.log_messages, f"ğŸ†• åµæ¸¬åˆ°æ–°æª”æ¡ˆï¼š{latest_file}ï¼Œç­‰å¾…å¤§å°ç©©å®š")
            return

        if current_size == self.last_file_size:
            self.file_stable_count += 1
            append_log(self.log_messages, f"ğŸ“ˆ æª”æ¡ˆå¤§å°é€£çºŒç¬¬ {self.file_stable_count} æ¬¡ç©©å®šï¼š{current_size} bytes")
        else:
            self.file_stable_count = 1
            self.last_file_size = current_size
            append_log(self.log_messages, f"ğŸ” æª”æ¡ˆå¤§å°è®Šå‹•ç‚º {current_size}ï¼Œé‡æ–°è¨ˆæ•¸")
            return

        if self.file_stable_count >= 2:
            self.last_processed_file = latest_file
            self.processed_files.add((latest_file, current_size, now))
            append_log(self.log_messages, f"ğŸš€ æª”æ¡ˆç©©å®šï¼Œæº–å‚™è‡ªå‹•åˆ†æï¼š{latest_file}")
            self._launch_auto_clean(latest_file)

    def scan_once(self) -> None:
        """ä¾› UI æ‰‹å‹•è§¸ç™¼ä¸€æ¬¡è³‡æ–™å¤¾æƒæã€‚"""
        append_log(self.log_messages, "ğŸ” æ‰‹å‹•è§¸ç™¼è³‡æ–™å¤¾æƒæ")
        self._inspect_folder(manual=True)

    # ==== è‡ªå‹•æ¸…æ´—èˆ‡æ¨è«– ====
    def _launch_auto_clean(self, file_path: str) -> None:
        if self.cleaning_lock.locked():
            append_log(self.log_messages, "â³ å‰ä¸€æ¬¡è‡ªå‹•åˆ†æå°šæœªå®Œæˆï¼Œç•¥éæœ¬æ¬¡è§¸ç™¼")
            return
        threading.Thread(target=self._run_auto_clean, args=(file_path,), daemon=True).start()

    def _run_auto_clean(self, file_path: str) -> None:
        with self.cleaning_lock:
            append_log(self.log_messages, "âš™ï¸ é–‹å§‹åŸ·è¡Œè‡ªå‹•æ¸…æ´—èˆ‡æ¨è«–æµç¨‹")
            binary_model = self.settings.get("binary_model_path", "").strip()
            multi_model = self.settings.get("model_path", "").strip()
            output_dir = self.settings.get("clean_csv_dir", "").strip()

            missing = []
            if not binary_model:
                missing.append("äºŒå…ƒæ¨¡å‹")
            if not multi_model:
                missing.append("å¤šå…ƒæ¨¡å‹")
            if not output_dir:
                missing.append("æ¸…æ´—è¼¸å‡ºè³‡æ–™å¤¾")
            if not os.path.exists(file_path):
                missing.append("log æª”æ¡ˆ")
            if missing:
                append_log(self.log_messages, f"âŒ è‡ªå‹•åˆ†æç¼ºå°‘å¿…è¦é …ç›®ï¼š{'ã€'.join(missing)}")
                return

            os.makedirs(output_dir, exist_ok=True)

            try:
                config = PipelineConfig(
                    raw_log_path=file_path,
                    binary_model_path=binary_model,
                    multiclass_model_path=multi_model,
                    output_dir=output_dir,
                    show_progress=False,
                )
                result = execute_pipeline(config)
                self.latest_result = result
                append_log(
                    self.log_messages,
                    f"âœ… è‡ªå‹•åˆ†æå®Œæˆï¼Œè¼¸å‡º CSVï¼š{result.get('binary_output_csv', '-')}",
                )
                if result.get("binary_output_pie"):
                    append_log(
                        self.log_messages,
                        f"ğŸ“Š äºŒå…ƒåœ“é¤…åœ–ï¼š{result.get('binary_output_pie')}",
                    )
                if result.get("multiclass_output_csv"):
                    append_log(
                        self.log_messages,
                        f"ğŸ“Š å¤šå…ƒçµæœï¼š{result.get('multiclass_output_csv')}",
                    )
                self._handle_auto_notification(result)
            except Exception as exc:  # pragma: no cover - ç›¡é‡é¿å…ä¸­æ–·
                append_log(self.log_messages, f"âŒ è‡ªå‹•åˆ†æå¤±æ•—ï¼š{exc}")

    def _handle_auto_notification(self, result: Dict[str, object]) -> None:
        """æ ¹æ“šå¤šå…ƒçµæœå•Ÿå‹•é€šçŸ¥æ¨¡çµ„ã€‚"""
        multi_csv = result.get("multiclass_output_csv")
        if not multi_csv:
            append_log(self.log_messages, "â„¹ï¸ æœ¬æ‰¹æ¬¡ç„¡æ”»æ“Šæµé‡ï¼Œæœªç”¢ç”Ÿå¤šå…ƒçµæœ")
            return
        if not isinstance(multi_csv, str) or not os.path.exists(multi_csv):
            append_log(self.log_messages, f"âš ï¸ æ‰¾ä¸åˆ°å¤šå…ƒçµæœæª”æ¡ˆï¼š{multi_csv}")
            return
        if multi_csv in self.notified_multiclass_files:
            append_log(self.log_messages, "â„¹ï¸ è©²å¤šå…ƒçµæœå·²æ¨æ’­éï¼Œç•¥éé‡è¤‡é€šçŸ¥")
            return

        try:
            df = pd.read_csv(multi_csv)
        except Exception as exc:
            append_log(self.log_messages, f"âŒ è®€å–å¤šå…ƒçµæœå¤±æ•—ï¼š{exc}")
            return

        if "Severity" not in df.columns:
            append_log(self.log_messages, "â„¹ï¸ å¤šå…ƒçµæœç¼ºå°‘ Severity æ¬„ä½ï¼Œç„¡æ³•è‡ªå‹•æ¨æ’­")
            return
        if not df["Severity"].astype(str).isin(["1", "2", "3"]).any():
            append_log(self.log_messages, "â„¹ï¸ æœ¬æ‰¹æ¬¡ç„¡é«˜é¢¨éšªäº‹ä»¶ï¼Œæœªå•Ÿå‹•æ¨æ’­")
            return

        settings = load_json(NOTIFIER_SETTINGS_FILE, DEFAULT_NOTIFIER_SETTINGS)
        append_log(self.log_messages, "ğŸ”” åµæ¸¬åˆ°é«˜é¢¨éšªäº‹ä»¶ï¼Œå•Ÿå‹•é€šçŸ¥æ¨¡çµ„")
        group_fields = settings.get("convergence_fields", ["source", "destination"])
        if not isinstance(group_fields, list):
            group_fields = ["source", "destination"]
        convergence_cfg = {
            "window_minutes": int(settings.get("convergence_window_minutes", 10) or 10),
            "group_fields": group_fields,
        }
        notification_pipeline(
            result_csv=multi_csv,
            gemini_api_key=settings.get("gemini_api_key", ""),
            line_channel_access_token=settings.get("line_channel_access_token", ""),
            line_webhook_url=settings.get("line_webhook_url", ""),
            discord_webhook_url=settings.get("discord_webhook_url", ""),
            ui_callback=lambda msg: append_log(self.log_messages, msg),
            convergence_config=convergence_cfg,
        )
        self.notified_multiclass_files.add(multi_csv)

    # ==== ä¾›è³‡æ–™æ¸…ç†æ¨¡çµ„å‘¼å«çš„æš«åœæ§åˆ¶ ====
    def pause(self) -> None:
        if not self.paused:
            self.paused = True
            append_log(self.log_messages, "â¸ï¸ ä¸»æµç¨‹å·²æš«åœï¼Œç­‰å¾…è³‡æ–™æ¸…ç†å®Œæˆ")

    def resume(self) -> None:
        if self.paused:
            self.paused = False
            append_log(self.log_messages, "â–¶ï¸ ä¸»æµç¨‹å·²æ¢å¾©")

    def manual_auto_clean(self, file_path: str) -> None:
        """ä¾› UI æ‰‹å‹•æŒ‡å®šæª”æ¡ˆä¸¦è§¸ç™¼è‡ªå‹•åˆ†æã€‚"""
        if not file_path:
            append_log(self.log_messages, "âš ï¸ è«‹è¼¸å…¥è¦åˆ†æçš„ log æª”æ¡ˆè·¯å¾‘")
            return
        if not os.path.exists(file_path):
            append_log(self.log_messages, f"âŒ æŒ‡å®šæª”æ¡ˆä¸å­˜åœ¨ï¼š{file_path}")
            return
        self.last_processed_file = file_path
        self._launch_auto_clean(file_path)


def get_log_monitor() -> LogMonitor:
    """å–å¾—å„²å­˜åœ¨ Streamlit session ä¸­çš„ LogMonitor å–®ä¾‹ã€‚"""
    if "cisco_log_monitor" not in st.session_state:
        st.session_state["cisco_log_monitor"] = LogMonitor()
    return st.session_state["cisco_log_monitor"]


def _persist_uploaded_model(uploaded_file, prefix: str) -> str:
    """å„²å­˜ä½¿ç”¨è€…ä¸Šå‚³çš„æ¨¡å‹æª”æ¡ˆä¸¦å›å‚³è·¯å¾‘ã€‚"""

    store = Path(tempfile.gettempdir()) / "cisco_model_store"
    store.mkdir(parents=True, exist_ok=True)
    original_name = getattr(uploaded_file, "name", f"{prefix}.pkl") or f"{prefix}.pkl"
    suffix = Path(original_name).suffix or ".pkl"
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    target_path = store / f"{prefix}_{timestamp}{suffix}"

    uploaded_file.seek(0)
    with open(target_path, "wb") as destination:
        destination.write(uploaded_file.getbuffer())

    return str(target_path)


def _render_path_preview(label: str, path: str, *, icon: str = "ğŸ“") -> None:
    """ä½¿ç”¨çµ±ä¸€æ¨£å¼å‘ˆç¾è·¯å¾‘è³‡è¨Šã€‚"""

    safe_label = html.escape(label)
    safe_icon = html.escape(icon)
    if path:
        display_path = html.escape(path)
        extra_class = ""
    else:
        display_path = "å°šæœªé¸æ“‡"
        extra_class = " path-preview--empty"

    st.markdown(
        f"""
        <div class="path-preview{extra_class}">
            <span class="path-preview__icon">{safe_icon}</span>
            <div class="path-preview__content">
                <span class="path-preview__label">{safe_label}</span>
                <span class="path-preview__path">{display_path}</span>
            </div>
        </div>
        """,
        unsafe_allow_html=True,
    )


def _persist_uploaded_manual_file(uploaded_file, monitor: LogMonitor) -> str:
    """å°‡ä½¿ç”¨è€…ä¸Šå‚³çš„æ‰‹å‹•åˆ†ææª”æ¡ˆè½åœ°ä¿å­˜ä¸¦å›å‚³è·¯å¾‘ã€‚"""

    base_dir = monitor.settings.get("save_dir", "")
    if base_dir and os.path.isdir(base_dir):
        target_dir = Path(base_dir)
    else:
        target_dir = Path(tempfile.gettempdir()) / "cisco_manual_logs"
    target_dir.mkdir(parents=True, exist_ok=True)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    original_name = Path(getattr(uploaded_file, "name", "uploaded.log") or "uploaded.log").name
    safe_name = original_name.replace(" ", "_")
    target_path = target_dir / f"manual_{timestamp}_{safe_name}"

    uploaded_file.seek(0)
    with open(target_path, "wb") as destination:
        destination.write(uploaded_file.getbuffer())

    return str(target_path)


def render_directory_selector(
    label: str,
    state_key: str,
    *,
    default: str = "",
    help_text: str | None = None,
) -> str:
    """æä¾›ä½¿ç”¨è€…ä»¥ç€è¦½æŒ‰éˆ•æˆ–æ‰‹å‹•è¼¸å…¥è¨­å®šè³‡æ–™å¤¾è·¯å¾‘ã€‚"""

    session_key = f"{state_key}_path"
    display_key = f"{session_key}_display"
    PATH_BROWSER_ROOT.mkdir(parents=True, exist_ok=True)

    if session_key not in st.session_state:
        st.session_state[session_key] = default.strip()
    if display_key not in st.session_state:
        st.session_state[display_key] = st.session_state[session_key]

    current_path = st.session_state[session_key]
    with st.container():
        st.text_input(
            label,
            value=st.session_state[display_key],
            key=display_key,
            disabled=True,
            placeholder="ä½¿ç”¨ä¸‹æ–¹ç€è¦½æŒ‰éˆ•é¸æ“‡è³‡æ–™å¤¾",
        )
        uploaded_files = st.file_uploader(
            "ç€è¦½",
            key=f"{session_key}_uploader",
            label_visibility="collapsed",
            accept_multiple_files=True,
            help=help_text
            or "é€éç€è¦½æŒ‰éˆ•æŒ‘é¸è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆï¼Œç³»çµ±æœƒå»ºç«‹å¯ç›£æ§çš„ç›®éŒ„ã€‚",
        )

    if uploaded_files:
        target_dir = PATH_BROWSER_ROOT / state_key
        if target_dir.exists():
            shutil.rmtree(target_dir)
        target_dir.mkdir(parents=True, exist_ok=True)
        for file_obj in uploaded_files:
            file_obj.seek(0)
            destination = target_dir / file_obj.name
            with open(destination, "wb") as handle:
                handle.write(file_obj.getbuffer())
        current_path = str(target_dir)
        st.session_state[session_key] = current_path
        st.session_state[display_key] = current_path
        st.success(f"å·²å»ºç«‹ç€è¦½è³‡æ–™å¤¾ï¼š{current_path}")

    manual_key = f"{session_key}_manual"
    with st.expander("éœ€è¦æ‰‹å‹•è¼¸å…¥è·¯å¾‘ï¼Ÿ", expanded=False):
        manual_value = st.text_input(
            "æ‰‹å‹•è¼¸å…¥è‡ªè¨‚è·¯å¾‘",
            value=st.session_state.get(manual_key, st.session_state[session_key]),
            key=manual_key,
        )
        manual_value = manual_value.strip()
        if manual_value and manual_value != st.session_state[session_key]:
            st.session_state[session_key] = manual_value
            st.session_state[display_key] = manual_value
            current_path = manual_value

    return st.session_state.get(session_key, current_path)


def app() -> None:
    """Streamlit ç‰ˆçš„ Log æ“·å–é é¢ã€‚"""
    monitor = get_log_monitor()

    st.title("ğŸ“„ Cisco Log æ“·å–èˆ‡è‡ªå‹•åˆ†æ")
    st.markdown("æ­¤é é¢è² è²¬ç›£æ§ ASA logã€åŸ·è¡Œè³‡æ–™æ¸…æ´—èˆ‡è‡ªå‹•æ¨æ’­ã€‚")

    st.session_state.setdefault("cisco_binary_model_path", monitor.settings.get("binary_model_path", ""))
    st.session_state.setdefault("cisco_multi_model_path", monitor.settings.get("model_path", ""))

    st.subheader("ç›£æ§è¨­å®š")
    with st.form("log_settings"):
        col_paths = st.columns(2)
        with col_paths[0]:
            save_dir = render_directory_selector(
                "log å„²å­˜è³‡æ–™å¤¾",
                "cisco_save_dir",
                default=monitor.settings.get("save_dir", ""),
                help_text="é€éç€è¦½æŒ‰éˆ•é¸æ“‡æ¬²ç›£æ§çš„è³‡æ–™å¤¾ï¼Œç³»çµ±æœƒå»ºç«‹å¯ä¾›ç›£è½çš„ç›®éŒ„ã€‚",
            )
        with col_paths[1]:
            clean_dir = render_directory_selector(
                "æ¸…æ´—è¼¸å‡ºè³‡æ–™å¤¾",
                "cisco_clean_dir",
                default=monitor.settings.get("clean_csv_dir", ""),
                help_text="é€éç€è¦½æŒ‰éˆ•å»ºç«‹æˆ–é¸æ“‡æ¸…æ´—è¼¸å‡ºçš„ç›®éŒ„ï¼Œäº¦å¯æ–¼ä¸‹æ–¹å±•é–‹æ‰‹å‹•èª¿æ•´ã€‚",
            )

        st.markdown("##### æ¨¡å‹æª”æ¡ˆ")
        current_binary = st.session_state.get(
            "cisco_binary_model_path", monitor.settings.get("binary_model_path", "")
        )
        binary_upload = st.file_uploader(
            "é¸æ“‡äºŒå…ƒæ¨¡å‹æª” (.pkl/.joblib)",
            type=["pkl", "joblib"],
            key="cisco_binary_model_upload",
            help="é€éç€è¦½æŒ‰éˆ•æŒ‘é¸äºŒå…ƒåˆ†é¡æ¨¡å‹ï¼Œå°‡è‡ªå‹•å„²å­˜ä¸¦å¥—ç”¨æ–¼ç›£æ§æµç¨‹ã€‚",
        )
        _render_path_preview("ç›®å‰ä½¿ç”¨çš„äºŒå…ƒæ¨¡å‹", current_binary, icon="ğŸ§ ")

        current_multi = st.session_state.get(
            "cisco_multi_model_path", monitor.settings.get("model_path", "")
        )
        multi_upload = st.file_uploader(
            "é¸æ“‡å¤šå…ƒæ¨¡å‹æª” (.pkl/.joblib)",
            type=["pkl", "joblib"],
            key="cisco_multi_model_upload",
            help="é€éç€è¦½æŒ‰éˆ•æŒ‘é¸å¤šå…ƒåˆ†é¡æ¨¡å‹ï¼Œå°‡è‡ªå‹•å„²å­˜ä¸¦å¥—ç”¨æ–¼ç›£æ§æµç¨‹ã€‚",
        )
        _render_path_preview("ç›®å‰ä½¿ç”¨çš„å¤šå…ƒæ¨¡å‹", current_multi, icon="ğŸ—‚ï¸")

        submitted = st.form_submit_button("ğŸ’¾ å„²å­˜è¨­å®š")
        if submitted:
            binary_path = current_binary
            multi_path = current_multi
            if binary_upload is not None:
                binary_path = _persist_uploaded_model(binary_upload, "binary_model")
            if multi_upload is not None:
                multi_path = _persist_uploaded_model(multi_upload, "multiclass_model")

            st.session_state["cisco_binary_model_path"] = binary_path
            st.session_state["cisco_multi_model_path"] = multi_path
            monitor.update_settings(
                save_dir=save_dir,
                binary_model_path=binary_path,
                model_path=multi_path,
                clean_csv_dir=clean_dir,
            )
            st.success("ç›£æ§è¨­å®šå·²æ›´æ–°")

    col1, col2, col3 = st.columns(3)
    if col1.button("â–¶ï¸ å•Ÿå‹•ç›£è½", use_container_width=True):
        monitor.start_listening()
    if col2.button("â¹ï¸ åœæ­¢ç›£è½", use_container_width=True):
        monitor.stop_listening()
    if col3.button("ğŸ” æ‰‹å‹•æƒæä¸€æ¬¡", use_container_width=True):
        monitor.scan_once()

    st.markdown("### æ‰‹å‹•åˆ†æ")
    manual_path = st.session_state.get("cisco_manual_uploaded_path", monitor.last_processed_file)
    uploaded_manual = st.file_uploader(
        "é¸æ“‡è¦åˆ†æçš„ log æª”æ¡ˆ",
        type=["log", "txt", "csv"],
        accept_multiple_files=False,
        help="é€éç€è¦½æŒ‰éˆ•æŒ‘é¸ ASA logï¼Œç³»çµ±æœƒè‡ªå‹•å„²å­˜ä¸¦å¸¶å…¥åˆ†ææµç¨‹ã€‚",
        key="cisco_manual_file_uploader",
    )
    if uploaded_manual is not None:
        manual_path = _persist_uploaded_manual_file(uploaded_manual, monitor)
        st.session_state["cisco_manual_uploaded_path"] = manual_path
        monitor.last_processed_file = manual_path
        st.success(f"å·²æº–å‚™æª”æ¡ˆï¼š{manual_path}")

    _render_path_preview("ç›®å‰é¸æ“‡çš„æª”æ¡ˆ", manual_path or "", icon="ğŸ“„")
    if not manual_path:
        st.caption("è«‹å…ˆé€éä¸Šæ–¹ç€è¦½æŒ‰éˆ•é¸æ“‡æ¬²åˆ†æçš„æª”æ¡ˆã€‚")

    if st.button("âš™ï¸ ç«‹å³åŸ·è¡Œè‡ªå‹•åˆ†æ", use_container_width=True):
        if manual_path:
            monitor.manual_auto_clean(manual_path)
        else:
            st.warning("è«‹å…ˆé¸æ“‡è¦åˆ†æçš„æª”æ¡ˆã€‚")

    st.markdown("### ç›£æ§ç‹€æ…‹")
    status = "ğŸŸ¢ ç›£è½ä¸­" if monitor.listening else "â›” å·²åœæ­¢"
    st.markdown(f"ç›®å‰ç‹€æ…‹ï¼š**{status}**")
    if monitor.latest_result:
        st.success("é¡¯ç¤ºæœ€æ–°è‡ªå‹•åˆ†æçµæœï¼š")
        st.json(monitor.latest_result)

    st.markdown("### åŸ·è¡Œæ—¥èªŒ")
    st.text_area(
        "åŸ·è¡Œæ—¥èªŒ",
        value="\n".join(monitor.log_messages),
        height=320,
    )
